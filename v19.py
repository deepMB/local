from typing import TypedDict, List, Optional, Any, Dict, Literal
from langgraph import StateGraph, END
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import BaseTool
from langchain_openai import ChatOpenAI
from langchain.agents import create_react_agent, AgentExecutor
from langchain_core.prompts import PromptTemplate
import json
import re
from pydantic import BaseModel, Field
import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient

# Agent State Definition
class AgentState(TypedDict):
    """
    Defines the shared state for the playbook-driven multi-agent system.
    """
    error_code: int
    sop_content: str
    
    playbook: Optional[Dict[str, Any]]
    """The structured JSON workflow graph generated by the Planning Agent."""

    execution_queue: List[str]
    """A queue of node IDs from the playbook that the Execution Agent needs to process."""
    
    data: Optional[Any]
    """Holds operational data, like the pandas DataFrame for error 935."""

    last_tool_result: Optional[Dict[str, Any]]
    """Stores the output from the last executed tool for conditional evaluation."""

    final_output: Dict[str, Any]
    """A dictionary to accumulate final results and statuses from the workflow."""

    current_step_id: Optional[str]
    """Currently executing step ID."""

    execution_status: str
    """Status of execution: 'planning', 'executing', 'completed', 'failed'."""

    execution_log: List[Dict[str, Any]]
    """Log of all executed steps and their results."""

    context_data: Dict[str, Any]
    """Accumulated context data from all executed steps."""


# Playbook Schema for structured output
class PlaybookStep(BaseModel):
    """Represents a single step in the playbook"""
    id: str = Field(description="Unique identifier for the step")
    name: str = Field(description="Human-readable name of the step")
    action: str = Field(description="The action to be performed")
    objective: str = Field(description="What the step is trying to achieve")
    success_criteria: str = Field(description="How to determine if the step was successful")
    failure_criteria: str = Field(description="How to determine if the step failed")
    next_steps: Dict[str, str] = Field(description="Next steps based on conditions (success, failure, etc.)")
    description: str = Field(description="Detailed description of what this step does")
    context_requirements: Optional[List[str]] = Field(description="What context/data is needed for this step")


class PlanningAgent:
    """
    Enhanced Planning Agent that creates ReAct-friendly playbooks.
    """
    
    def __init__(self, llm: ChatOpenAI, available_tools: List[BaseTool]):
        self.llm = llm
        self.available_tools = available_tools
        self.tool_names = [tool.name for tool in available_tools]
        
    def analyze_sop(self, state: AgentState) -> AgentState:
        """
        Analyzes the SOP content and creates a structured playbook optimized for ReAct execution.
        """
        print("🔍 Planning Agent: Starting SOP analysis...")
        
        sop_content = state["sop_content"]
        error_code = state["error_code"]
        
        # Update execution status
        state["execution_status"] = "planning"
        state["context_data"] = {}
        
        # Create detailed tool descriptions
        tool_descriptions = []
        for tool in self.available_tools:
            tool_descriptions.append(f"- {tool.name}: {tool.description}")
        
        # Enhanced system prompt for ReAct-optimized planning
        system_prompt = f"""
        You are an expert SOP analyzer that creates ReAct-optimized playbooks for intelligent execution agents.
        
        Your task is to analyze the SOP and create a JSON playbook where each step provides enough context 
        for a ReAct agent to intelligently choose tools and make decisions based on tool outcomes.
        
        AVAILABLE TOOLS:
        {chr(10).join(tool_descriptions)}
        
        CRITICAL DESIGN PRINCIPLES:
        1. Each step should define OBJECTIVES, not specific tools to use
        2. Include clear SUCCESS and FAILURE criteria that a ReAct agent can evaluate
        3. Steps should be tool-agnostic - let the ReAct agent choose the best tool for the objective
        4. Provide rich context about what each step is trying to achieve
        5. Design conditional flows that can handle multiple scenarios
        
        Guidelines for ReAct-optimized playbooks:
        1. Focus on WHAT needs to be achieved, not HOW to achieve it
        2. Provide clear success/failure criteria for each step
        3. Include context requirements (what data/information is needed)
        4. Design steps that allow the ReAct agent to reason about tool selection
        5. Create robust error handling and alternative paths
        6. Each step should be self-contained with clear objectives
        
        Error Code Context: {error_code}
        
        IMPORTANT: Return ONLY the JSON playbook, no other text or explanations.
        """
        
        human_prompt = f"""
        Create a ReAct-optimized JSON playbook for this SOP:
        
        {sop_content}
        
        Required JSON structure:
        {{
            "name": "Playbook name",
            "description": "What this playbook accomplishes",
            "start_step": "first_step_id",
            "steps": {{
                "step_id": {{
                    "id": "step_id",
                    "name": "Step name",
                    "action": "achieve_objective|evaluate_condition|notify|end",
                    "objective": "What this step needs to accomplish",
                    "success_criteria": "How to determine success",
                    "failure_criteria": "How to determine failure", 
                    "next_steps": {{"success": "next_id", "failure": "error_id", "default": "END"}},
                    "description": "Detailed description",
                    "context_requirements": ["required_data1", "required_data2"]
                }}
            }}
        }}
        
        RULES:
        1. Use "END" for terminal steps
        2. Focus on objectives, not specific tools
        3. Include rich success/failure criteria
        4. All referenced step IDs must exist
        5. Make it ReAct agent friendly - provide reasoning context
        """
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt)
        ]
        
        try:
            response = self.llv.invoke(messages)
            playbook_json = self._extract_json_from_response(response.content)
            playbook = self._validate_and_structure_playbook(playbook_json)
            
            state["playbook"] = playbook
            state["execution_queue"] = [playbook["start_step"]] if playbook else []
            state["execution_status"] = "ready_to_execute"
            
            print(f"✅ Planning Agent: Created ReAct-optimized playbook with {len(playbook.get('steps', {}))} steps")
            print(f"📋 Start step: {playbook['start_step']}")
            
        except Exception as e:
            print(f"❌ Planning Agent Error: {str(e)}")
            state["playbook"] = None
            state["execution_queue"] = []
            state["execution_status"] = "failed"
            state["final_output"]["error"] = f"Planning failed: {str(e)}"
        
        return state
    
    def _extract_json_from_response(self, response_content: str) -> Dict[str, Any]:
        """Extracts JSON from the LLM response."""
        json_pattern = r'```json\s*(.*?)\s*```'
        json_match = re.search(json_pattern, response_content, re.DOTALL)
        
        if json_match:
            json_str = json_match.group(1)
        else:
            json_str = response_content.strip()
        
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            json_start = response_content.find('{')
            json_end = response_content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_str = response_content[json_start:json_end]
                return json.loads(json_str)
            else:
                raise ValueError(f"Could not extract valid JSON from response: {str(e)}")
    
    def _validate_and_structure_playbook(self, playbook_json: Dict[str, Any]) -> Dict[str, Any]:
        """Validates the playbook structure."""
        if "steps" not in playbook_json:
            raise ValueError("Playbook must contain 'steps' field")
        
        if "start_step" not in playbook_json:
            playbook_json["start_step"] = list(playbook_json["steps"].keys())[0]
        
        if playbook_json["start_step"] not in playbook_json["steps"]:
            raise ValueError(f"Start step '{playbook_json['start_step']}' not found in steps")
        
        # Ensure each step has required fields
        for step_id, step_data in playbook_json["steps"].items():
            if "id" not in step_data:
                step_data["id"] = step_id
            if "next_steps" not in step_data:
                step_data["next_steps"] = {"default": "END"}
            if "action" not in step_data:
                step_data["action"] = "achieve_objective"
            if "success_criteria" not in step_data:
                step_data["success_criteria"] = "Operation completed successfully"
            if "failure_criteria" not in step_data:
                step_data["failure_criteria"] = "Operation failed or error occurred"
            if "context_requirements" not in step_data:
                step_data["context_requirements"] = []
                
            # Normalize end step references
            if "next_steps" in step_data:
                normalized_next_steps = {}
                for condition, next_step in step_data["next_steps"].items():
                    if isinstance(next_step, str) and next_step.lower() in ['end', 'stop', 'finish', 'complete']:
                        normalized_next_steps[condition] = "END"
                    else:
                        normalized_next_steps[condition] = next_step
                step_data["next_steps"] = normalized_next_steps
        
        return playbook_json


class ExecutionAgent:
    """
    Enhanced Execution Agent that uses ReAct pattern for intelligent tool selection and decision making.
    """
    
    def __init__(self, llm: ChatOpenAI, available_tools: List[BaseTool]):
        self.llm = llm
        self.available_tools = available_tools
        self.tool_dict = {tool.name: tool for tool in available_tools}
        
        # Get tool names for the prompt
        tool_names = [tool.name for tool in available_tools]
        
        # Enhanced ReAct prompt for workflow execution with better argument formatting
        react_template = """You are an intelligent workflow execution agent. You have access to the following tools:

{tools}

Your role is to execute workflow steps by:
1. Understanding the objective of each step
2. Choosing the most appropriate tool(s) to achieve the objective
3. Evaluating tool results against success/failure criteria
4. Making decisions about next steps based on outcomes

Use the following format:

Question: the workflow step you need to execute
Thought: analyze the objective and determine what needs to be done
Action: choose the most appropriate action from [{tool_names}]
Action Input: the input to the action
Observation: analyze the result of the action
... (repeat Thought/Action/Action Input/Observation as needed)
Thought: evaluate if the step objective has been achieved based on success/failure criteria
Final Answer: provide a clear assessment of whether the step succeeded or failed, with reasoning

CRITICAL RULES FOR ACTION INPUT:
1. For tools with single parameters, provide just the value (e.g., for errorcode parameter, use: 935)
2. For tools with multiple parameters, provide a valid JSON object (e.g., {{"errorcode": 935, "required_fields": ["field1", "field2"]}})
3. Ensure integer parameters are passed as numbers, not strings
4. Ensure string parameters are properly quoted when in JSON
5. Ensure list parameters are properly formatted as JSON arrays when in JSON
6. Match the exact parameter names expected by the tool

ADDITIONAL RULES:
1. If no suitable tool exists for an objective, respond with "NO_SUITABLE_TOOL"
2. If a tool fails or returns unsatisfactory results, try alternative approaches if available
3. Always evaluate results against the provided success/failure criteria
4. If you cannot achieve the objective after trying available tools, respond with "OBJECTIVE_FAILED"

Begin!

Question: {input}
Thought:{agent_scratchpad}"""

        self.react_prompt = PromptTemplate.from_template(react_template)
        
        # Create enhanced ReAct agent
        self.react_agent = create_react_agent(
            llm=self.llm,
            tools=self.available_tools,
            prompt=self.react_prompt
        )
        
        # Create agent executor with better error handling
        self.agent_executor = AgentExecutor(
            agent=self.react_agent,
            tools=self.available_tools,
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=5,  # Reduced to prevent excessive iterations
            early_stopping_method="generate"
        )
        
    async def execute_step(self, state: AgentState) -> AgentState:
        """
        Enhanced step execution using ReAct pattern for intelligent decision making.
        """
        if not state["execution_queue"]:
            state["execution_status"] = "completed"
            return state
            
        current_step_id = state["execution_queue"][0]
        state["current_step_id"] = current_step_id
        
        print(f"🔧 Execution Agent: Executing step '{current_step_id}'")
        
        # Handle END step
        if current_step_id.upper() == "END":
            state["execution_status"] = "completed"
            state["execution_queue"] = []
            print("✅ Execution Agent: Workflow completed successfully")
            return state
        
        # Get current step details
        if not state["playbook"] or current_step_id not in state["playbook"]["steps"]:
            if current_step_id.lower() in ['end', 'stop', 'finish', 'complete']:
                state["execution_status"] = "completed"
                state["execution_queue"] = []
                print("✅ Execution Agent: Workflow completed successfully")
                return state
            else:
                state["execution_status"] = "failed"
                state["final_output"]["error"] = f"Step '{current_step_id}' not found in playbook"
                return state
            
        current_step = state["playbook"]["steps"][current_step_id]
        
        try:
            # Execute the step using ReAct pattern
            if current_step["action"] == "achieve_objective":
                result = await self._execute_objective_step(current_step, state)
            elif current_step["action"] == "evaluate_condition":
                result = await self._evaluate_condition_step(current_step, state)
            elif current_step["action"] == "notify":
                result = await self._notify_step(current_step, state)
            else:
                result = {"status": "success", "message": f"Executed step: {current_step['name']}"}
            
            # Check for critical failures
            if result.get("critical_failure"):
                print(f"🛑 Critical failure in step '{current_step_id}': {result.get('message')}")
                state["execution_status"] = "failed"
                state["final_output"]["error"] = result.get("message", "Critical failure occurred")
                return state
            
            # Log the execution
            log_entry = {
                "step_id": current_step_id,
                "step_name": current_step.get("name", ""),
                "objective": current_step.get("objective", ""),
                "result": result,
                "timestamp": "now"
            }
            
            if "execution_log" not in state:
                state["execution_log"] = []
            state["execution_log"].append(log_entry)
            
            # Update context data
            if result.get("context_data"):
                state["context_data"].update(result["context_data"])
            
            # Store last tool result
            state["last_tool_result"] = result
            
            # Determine next step using enhanced logic
            next_step_id = self._determine_next_step(current_step, result)
            
            # Update execution queue
            state["execution_queue"] = state["execution_queue"][1:]
            
            if next_step_id and next_step_id != "END":
                state["execution_queue"].insert(0, next_step_id)
            
            print(f"✅ Step '{current_step_id}' completed. Next: '{next_step_id}'")
            
        except Exception as e:
            print(f"❌ Execution Agent Error in step '{current_step_id}': {str(e)}")
            state["execution_status"] = "failed"
            state["final_output"]["error"] = f"Execution failed at step '{current_step_id}': {str(e)}"
            
        return state
    
    async def _execute_objective_step(self, step: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """
        Execute an objective-based step using ReAct agent for intelligent tool selection.
        """
        objective = step.get("objective", "")
        success_criteria = step.get("success_criteria", "")
        failure_criteria = step.get("failure_criteria", "")
        context_requirements = step.get("context_requirements", [])
        step_description = step.get("description", "")
        
        # Prepare context information
        available_context = {}
        for req in context_requirements:
            if req in state["context_data"]:
                available_context[req] = state["context_data"][req]
        
        # Get error code from state for tool calls that need it
        error_code = state.get("error_code", 0)
        
        # Create comprehensive question for ReAct agent
        question = f"""
        WORKFLOW STEP EXECUTION:
        
        Step: {step.get('name', 'Unnamed Step')}
        Objective: {objective}
        Description: {step_description}
        
        SUCCESS CRITERIA: {success_criteria}
        FAILURE CRITERIA: {failure_criteria}
        
        ERROR CODE CONTEXT: {error_code} (Use this value for any errorcode parameters)
        
        Available Context:
        {json.dumps(available_context, indent=2) if available_context else "No specific context required"}
        
        Previous Step Result:
        {json.dumps(state.get('last_tool_result', {}), indent=2)}
        
        IMPORTANT PARAMETER FORMATTING:
        - For errorcode parameters: Use the integer {error_code}
        - For single parameter tools: Provide just the value
        - For multi-parameter tools: Use JSON format like {{"param1": value1, "param2": value2}}
        
        Your task is to achieve the objective using the available tools. 
        Evaluate your results against the success/failure criteria.
        If you cannot find suitable tools or achieve the objective, clearly state this in your Final Answer.
        """
        
        try:
            print(f"🤖 ReAct Agent executing objective: {objective}")
            
            # Use ReAct agent to execute the step
            result = await self.agent_executor.invoke({"input": question})
            agent_output = result.get('output', '') if hasattr(result, 'get') else str(result)
            
            print(f"🔍 ReAct Agent Output: {agent_output}")
            
            # Enhanced result analysis
            result_analysis = self._analyze_agent_result(agent_output, success_criteria, failure_criteria)
            
            # Check for critical conditions
            if "NO_SUITABLE_TOOL" in agent_output:
                return {
                    "status": "failure",
                    "critical_failure": True,
                    "message": f"No suitable tool available for objective: {objective}",
                    "agent_output": agent_output,
                    "reason": "tool_unavailable"
                }
            
            if "OBJECTIVE_FAILED" in agent_output:
                return {
                    "status": "failure",
                    "critical_failure": True,
                    "message": f"Failed to achieve objective: {objective}",
                    "agent_output": agent_output,
                    "reason": "objective_not_achieved"
                }
            
            # Extract tool results from intermediate steps
            tool_results = []
            context_data = {}
            
            if hasattr(result, 'get') and 'intermediate_steps' in result:
                for step_result in result['intermediate_steps']:
                    if len(step_result) > 1:
                        tool_name = step_result[0].tool if hasattr(step_result[0], 'tool') else 'unknown'
                        tool_output = step_result[1]
                        tool_results.append({
                            "tool": tool_name,
                            "output": tool_output
                        })
                        # Extract potential context data
                        context_data[f"{tool_name}_result"] = tool_output
            
            return {
                "status": result_analysis["status"],
                "objective": objective,
                "success_criteria": success_criteria,
                "failure_criteria": failure_criteria,
                "tool_results": tool_results,
                "agent_output": agent_output,
                "analysis": result_analysis,
                "context_data": context_data,
                "message": result_analysis["message"]
            }
            
        except Exception as e:
            print(f"❌ Error in ReAct execution: {str(e)}")
            return {
                "status": "failure",
                "critical_failure": True,
                "objective": objective,
                "error": str(e),
                "message": f"ReAct execution failed: {str(e)}"
            }
    
    def _analyze_agent_result(self, agent_output: str, success_criteria: str, failure_criteria: str) -> Dict[str, Any]:
        """
        Analyze the ReAct agent output to determine success/failure.
        """
        output_lower = agent_output.lower()
        
        # Enhanced success/failure detection
        success_indicators = ['success', 'completed', 'achieved', 'valid', 'connected', 'verified', 'passed']
        failure_indicators = ['failed', 'error', 'invalid', 'disconnected', 'rejected', 'unable', 'cannot']
        
        # Check for explicit success/failure in output
        success_count = sum(1 for indicator in success_indicators if indicator in output_lower)
        failure_count = sum(1 for indicator in failure_indicators if indicator in output_lower)
        
        # Use LLM for more sophisticated analysis if indicators are ambiguous
        if abs(success_count - failure_count) <= 1:  # Ambiguous result
            try:
                analysis_prompt = f"""
                Analyze the following agent output and determine if it indicates success or failure:
                
                Agent Output: {agent_output}
                Success Criteria: {success_criteria}
                Failure Criteria: {failure_criteria}
                
                Respond with exactly: SUCCESS or FAILURE followed by a brief explanation.
                """
                
                analysis_result = self.llm.invoke([HumanMessage(content=analysis_prompt)])
                analysis_text = analysis_result.content.upper()
                
                if "SUCCESS" in analysis_text:
                    return {
                        "status": "success",
                        "message": "Objective achieved based on LLM analysis",
                        "llm_analysis": analysis_result.content
                    }
                else:
                    return {
                        "status": "failure", 
                        "message": "Objective not achieved based on LLM analysis",
                        "llm_analysis": analysis_result.content
                    }
            except:
                pass  # Fall back to simple analysis
        
        # Simple indicator-based analysis
        if success_count > failure_count:
            return {
                "status": "success",
                "message": f"Objective achieved (success indicators: {success_count})",
                "confidence": "indicator_based"
            }
        else:
            return {
                "status": "failure",
                "message": f"Objective not achieved (failure indicators: {failure_count})",
                "confidence": "indicator_based"
            }
    
    async def _evaluate_condition_step(self, step: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """Enhanced condition evaluation using ReAct reasoning."""
        objective = step.get("objective", "")
        last_result = state.get("last_tool_result", {})
        context_data = state.get("context_data", {})
        
        evaluation_question = f"""
        CONDITION EVALUATION TASK:
        
        Objective: {objective}
        
        Previous Step Result: {json.dumps(last_result, indent=2)}
        Available Context: {json.dumps(context_data, indent=2)}
        
        Your task is to evaluate whether the condition/objective is met based on the available information.
        Use the available tools if you need to gather additional information.
        
        Provide a clear TRUE or FALSE determination with reasoning.
        """
        
        try:
            result = await self.agent_executor.invoke({"input": evaluation_question})
            agent_output = result.get('output', '') if hasattr(result, 'get') else str(result)
            
            # Determine condition result
            condition_met = self._determine_condition_result(agent_output)
            
            return {
                "status": "success" if condition_met else "failure",
                "objective": objective,
                "condition_met": condition_met,
                "agent_output": agent_output,
                "message": f"Condition evaluation: {condition_met}"
            }
            
        except Exception as e:
            return {
                "status": "failure",
                "critical_failure": True,
                "objective": objective,
                "error": str(e),
                "message": f"Condition evaluation failed: {str(e)}"
            }
    
    def _determine_condition_result(self, agent_output: str) -> bool:
        """Determine if condition is met from agent output."""
        output_lower = agent_output.lower()
        
        # Look for explicit true/false statements
        if "true" in output_lower and "false" not in output_lower:
            return True
        if "false" in output_lower and "true" not in output_lower:
            return False
        
        # Look for positive/negative indicators
        positive_indicators = ['yes', 'confirmed', 'met', 'satisfied', 'passed', 'valid']
        negative_indicators = ['no', 'denied', 'not met', 'failed', 'invalid', 'unsatisfied']
        
        positive_count = sum(1 for indicator in positive_indicators if indicator in output_lower)
        negative_count = sum(1 for indicator in negative_indicators if indicator in output_lower)
        
        return positive_count > negative_count
    
    async def _notify_step(self, step: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """Enhanced notification step."""
        objective = step.get("objective", "")
        description = step.get("description", "")
        
        print(f"📢 Notification: {objective}")
        print(f"   Details: {description}")
        
        return {
            "status": "success",
            "objective": objective,
            "message": f"Notification completed: {objective}"
        }
    
    def _determine_next_step(self, current_step: Dict[str, Any], result: Dict[str, Any]) -> str:
        """Enhanced next step determination."""
        next_steps = current_step.get("next_steps", {})
        result_status = result.get("status", "unknown")
        
        # Check for critical failure - should end workflow
        if result.get("critical_failure"):
            return "END"
        
        # Try to find next step based on result status
        next_step = None
        if result_status in next_steps:
            next_step = next_steps[result_status]
        elif "default" in next_steps:
            next_step = next_steps["default"]
        else:
            next_step = "END"
        
        # Normalize end step variations
        if next_step and next_step.lower() in ['end', 'stop', 'finish', 'complete']:
            next_step = "END"
            
        return next_step
    


def create_multi_agent_workflow(llm: ChatOpenAI, available_tools: List[BaseTool]) -> StateGraph:
    """
    Creates the enhanced multi-agent workflow with ReAct-powered execution.
    """
    planning_agent = PlanningAgent(llm, available_tools)
    execution_agent = ExecutionAgent(llm, available_tools)
    
    # Create the state graph
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("planning", planning_agent.analyze_sop)
    workflow.add_node("execution", execution_agent.execute_step)
    
    # Set entry point
    workflow.set_entry_point("planning")
    
    # Enhanced routing logic
    def route_after_planning(state: AgentState) -> Literal["execution", "__end__"]:
        """Route after planning phase."""
        if state["execution_status"] == "ready_to_execute" and state["execution_queue"]:
            return "execution"
        else:
            return "__end__"
    
    def route_after_execution(state: AgentState) -> Literal["execution", "__end__"]:
        """Route after execution phase."""
        status = state["execution_status"]
        
        if status == "completed":
            print("🎉 Workflow completed successfully!")
            return "__end__"
        elif status == "failed":
            print("💥 Workflow failed - terminating")
            return "__end__"
        elif state["execution_queue"]:
            return "execution"
        else:
            return "__end__"
    
    # Add conditional edges
    workflow.add_conditional_edges(
        "planning",
        route_after_planning,
        {
            "execution": "execution",
            "__end__": END
        }
    )
    
    workflow.add_conditional_edges(
        "execution",
        route_after_execution,
        {
            "execution": "execution",
            "__end__": END
        }
    )
    
    return workflow


# Example usage and testing
if __name__ == "__main__":
    from langchain_core.tools import tool
    
    # Enhanced mock tools for demonstration
    async def main():
        # Initialize MCP client
        client = MultiServerMCPClient({
            "weather": {
            # make sure you start your weather server on port 8000
            "url": "http://localhost:8000/mcp/",
            "transport": "streamable_http",
        }
        })
        
        # Initialize LLM (replace with your preferred LLM)
        llm = ChatOpenAI(model="gpt-4", temperature=0)
        
        # Get tools from MCP servers
        tools = await client.get_tools()
        # Create the enhanced multi-agent workflow
        workflow = create_multi_agent_workflow(llm, tools)
    
        # Compile the graph
        app = workflow.compile()
    
        # Enhanced example SOP content with more complex logic
        sample_sop = """
        SOP for Error Code 935 - Data Validation Failure
        
        OBJECTIVE: Resolve data validation issues and ensure system integrity
        
        WORKFLOW:
        1. VALIDATE DATA SOURCE: Check the integrity of 'customer_data' source
        - IF validation passes: Proceed to step 6 (success notification)
        - IF validation fails: Continue to step 2
        
        2. DIAGNOSE CONNECTION: Verify database connectivity to 'main_db'
        - IF connection is healthy: Proceed to step 3
        - IF connection fails: Send error notification and END
        
        3. REPAIR DATA: Execute data repair using 'cleanup' method on 'customer_data'
        - IF repair succeeds: Proceed to step 4
        - IF repair fails: Send error notification and END
        
        4. RE-VALIDATE: Re-check the 'customer_data' after repair
        - IF validation now passes: Proceed to step 5
        - IF validation still fails: Send failure notification and END
        
        5. EXECUTE MAINTENANCE: Run maintenance query 'UPDATE customers SET status = active WHERE status IS NULL'
        - IF query succeeds: Proceed to step 6
        - IF query fails: Send error notification and END
        
        6. SUCCESS NOTIFICATION: Send success notification to 'admin@company.com'
        - Always END after this step
        
        ERROR HANDLING: For any unexpected errors, send detailed error notification to 'admin@company.com' and END
        """
        
        # Test the enhanced multi-agent system
        initial_state: AgentState = {
            "error_code": 935,
            "sop_content": sample_sop,
            "playbook": None,
            "execution_queue": [],
            "data": None,
            "last_tool_result": None,
            "final_output": {},
            "current_step_id": None,
            "execution_status": "planning",
            "execution_log": [],
            "context_data": {}
        }
        
        # Run the enhanced multi-agent workflow
        result = await app.invoke(initial_state)
            
    try:       
    # Run the async main function
        asyncio.run(main())
    except Exception as e:
        print(f"❌ Error running enhanced multi-agent workflow: {str(e)}")
        import traceback
        traceback.print_exc()