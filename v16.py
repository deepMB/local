from typing import TypedDict, List, Optional, Any, Dict, Literal
from langgraph import StateGraph, END
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import BaseTool
from langchain_openai import ChatOpenAI
from langchain.agents import create_react_agent, AgentExecutor
from langchain_core.prompts import PromptTemplate
import json
import re
from pydantic import BaseModel, Field

# Agent State Definition
class AgentState(TypedDict):
    """
    Defines the shared state for the playbook-driven multi-agent system.
    """
    error_code: int
    sop_content: str
    
    playbook: Optional[Dict[str, Any]]
    """The structured JSON workflow graph generated by the Planning Agent."""

    execution_queue: List[str]
    """A queue of node IDs from the playbook that the Execution Agent needs to process."""
    
    data: Optional[Any]
    """Holds operational data, like the pandas DataFrame for error 935."""

    last_tool_result: Optional[Dict[str, Any]]
    """Stores the output from the last executed tool for conditional evaluation."""

    final_output: Dict[str, Any]
    """A dictionary to accumulate final results and statuses from the workflow."""

    current_step_id: Optional[str]
    """Currently executing step ID."""

    execution_status: str
    """Status of execution: 'planning', 'executing', 'completed', 'failed'."""

    execution_log: List[Dict[str, Any]]
    """Log of all executed steps and their results."""

    context_data: Dict[str, Any]
    """Accumulated context data from all executed steps."""


# Playbook Schema for structured output
class PlaybookStep(BaseModel):
    """Represents a single step in the playbook"""
    id: str = Field(description="Unique identifier for the step")
    name: str = Field(description="Human-readable name of the step")
    action: str = Field(description="The action to be performed")
    objective: str = Field(description="What the step is trying to achieve")
    success_criteria: str = Field(description="How to determine if the step was successful")
    failure_criteria: str = Field(description="How to determine if the step failed")
    next_steps: Dict[str, str] = Field(description="Next steps based on conditions (success, failure, etc.)")
    description: str = Field(description="Detailed description of what this step does")
    context_requirements: Optional[List[str]] = Field(description="What context/data is needed for this step")


class PlanningAgent:
    """
    Enhanced Planning Agent that creates ReAct-friendly playbooks.
    """
    
    def __init__(self, llm: ChatOpenAI, available_tools: List[BaseTool]):
        self.llm = llm
        self.available_tools = available_tools
        self.tool_names = [tool.name for tool in available_tools]
        
    def analyze_sop(self, state: AgentState) -> AgentState:
        """
        Analyzes the SOP content and creates a structured playbook optimized for ReAct execution.
        """
        print("🔍 Planning Agent: Starting SOP analysis...")
        
        sop_content = state["sop_content"]
        error_code = state["error_code"]
        
        # Update execution status
        state["execution_status"] = "planning"
        state["context_data"] = {}
        
        # Create detailed tool descriptions
        tool_descriptions = []
        for tool in self.available_tools:
            tool_descriptions.append(f"- {tool.name}: {tool.description}")
        
        # Enhanced system prompt for ReAct-optimized planning
        system_prompt = f"""
        You are an expert SOP analyzer that creates ReAct-optimized playbooks for intelligent execution agents.
        
        Your task is to analyze the SOP and create a JSON playbook where each step provides enough context 
        for a ReAct agent to intelligently choose tools and make decisions based on tool outcomes.
        
        AVAILABLE TOOLS:
        {chr(10).join(tool_descriptions)}
        
        CRITICAL DESIGN PRINCIPLES:
        1. Each step should define OBJECTIVES, not specific tools to use
        2. Include clear SUCCESS and FAILURE criteria that a ReAct agent can evaluate
        3. Steps should be tool-agnostic - let the ReAct agent choose the best tool for the objective
        4. Provide rich context about what each step is trying to achieve
        5. Design conditional flows that can handle multiple scenarios
        
        Guidelines for ReAct-optimized playbooks:
        1. Focus on WHAT needs to be achieved, not HOW to achieve it
        2. Provide clear success/failure criteria for each step
        3. Include context requirements (what data/information is needed)
        4. Design steps that allow the ReAct agent to reason about tool selection
        5. Create robust error handling and alternative paths
        6. Each step should be self-contained with clear objectives
        
        Error Code Context: {error_code}
        
        IMPORTANT: Return ONLY the JSON playbook, no other text or explanations.
        """
        
        human_prompt = f"""
        Create a ReAct-optimized JSON playbook for this SOP:
        
        {sop_content}
        
        Required JSON structure:
        {{
            "name": "Playbook name",
            "description": "What this playbook accomplishes",
            "start_step": "first_step_id",
            "steps": {{
                "step_id": {{
                    "id": "step_id",
                    "name": "Step name",
                    "action": "achieve_objective|evaluate_condition|notify|end",
                    "objective": "What this step needs to accomplish",
                    "success_criteria": "How to determine success",
                    "failure_criteria": "How to determine failure", 
                    "next_steps": {{"success": "next_id", "failure": "error_id", "default": "END"}},
                    "description": "Detailed description",
                    "context_requirements": ["required_data1", "required_data2"]
                }}
            }}
        }}
        
        RULES:
        1. Use "END" for terminal steps
        2. Focus on objectives, not specific tools
        3. Include rich success/failure criteria
        4. All referenced step IDs must exist
        5. Make it ReAct agent friendly - provide reasoning context
        """
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt)
        ]
        
        try:
            response = self.llv.invoke(messages)
            playbook_json = self._extract_json_from_response(response.content)
            playbook = self._validate_and_structure_playbook(playbook_json)
            
            state["playbook"] = playbook
            state["execution_queue"] = [playbook["start_step"]] if playbook else []
            state["execution_status"] = "ready_to_execute"
            
            print(f"✅ Planning Agent: Created ReAct-optimized playbook with {len(playbook.get('steps', {}))} steps")
            print(f"📋 Start step: {playbook['start_step']}")
            
        except Exception as e:
            print(f"❌ Planning Agent Error: {str(e)}")
            state["playbook"] = None
            state["execution_queue"] = []
            state["execution_status"] = "failed"
            state["final_output"]["error"] = f"Planning failed: {str(e)}"
        
        return state
    
    def _extract_json_from_response(self, response_content: str) -> Dict[str, Any]:
        """Extracts JSON from the LLM response."""
        json_pattern = r'```json\s*(.*?)\s*```'
        json_match = re.search(json_pattern, response_content, re.DOTALL)
        
        if json_match:
            json_str = json_match.group(1)
        else:
            json_str = response_content.strip()
        
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            json_start = response_content.find('{')
            json_end = response_content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_str = response_content[json_start:json_end]
                return json.loads(json_str)
            else:
                raise ValueError(f"Could not extract valid JSON from response: {str(e)}")
    
    def _validate_and_structure_playbook(self, playbook_json: Dict[str, Any]) -> Dict[str, Any]:
        """Validates the playbook structure."""
        if "steps" not in playbook_json:
            raise ValueError("Playbook must contain 'steps' field")
        
        if "start_step" not in playbook_json:
            playbook_json["start_step"] = list(playbook_json["steps"].keys())[0]
        
        if playbook_json["start_step"] not in playbook_json["steps"]:
            raise ValueError(f"Start step '{playbook_json['start_step']}' not found in steps")
        
        # Ensure each step has required fields
        for step_id, step_data in playbook_json["steps"].items():
            if "id" not in step_data:
                step_data["id"] = step_id
            if "next_steps" not in step_data:
                step_data["next_steps"] = {"default": "END"}
            if "action" not in step_data:
                step_data["action"] = "achieve_objective"
            if "success_criteria" not in step_data:
                step_data["success_criteria"] = "Operation completed successfully"
            if "failure_criteria" not in step_data:
                step_data["failure_criteria"] = "Operation failed or error occurred"
            if "context_requirements" not in step_data:
                step_data["context_requirements"] = []
                
            # Normalize end step references
            if "next_steps" in step_data:
                normalized_next_steps = {}
                for condition, next_step in step_data["next_steps"].items():
                    if isinstance(next_step, str) and next_step.lower() in ['end', 'stop', 'finish', 'complete']:
                        normalized_next_steps[condition] = "END"
                    else:
                        normalized_next_steps[condition] = next_step
                step_data["next_steps"] = normalized_next_steps
        
        return playbook_json


class ExecutionAgent:
    """
    Enhanced Execution Agent that uses ReAct pattern for intelligent tool selection and decision making.
    """
    
    def __init__(self, llm: ChatOpenAI, available_tools: List[BaseTool]):
        self.llm = llm
        self.available_tools = available_tools
        self.tool_dict = {tool.name: tool for tool in available_tools}
        
        # Get detailed tool schemas for better prompt construction
        tool_schemas = {}
        for tool in available_tools:
            tool_schema = getattr(tool, 'args_schema', None)
            if tool_schema:
                schema_info = tool_schema.schema()
                tool_schemas[tool.name] = schema_info
        
        # Enhanced ReAct prompt with better tool parameter handling
        react_template = """You are an intelligent workflow execution agent. You have access to the following tools:

{tools}

CRITICAL TOOL USAGE RULES:
1. For tools with integer parameters: Use plain integers (e.g., 935, not "935" or "errorcode-935")
2. For tools with list parameters: Use proper Python list syntax (e.g., ["item1", "item2"])
3. For tools with string parameters: Use plain strings without extra quotes
4. NEVER use JSON format for Action Input - use the exact parameter values directly
5. If a tool expects multiple parameters, provide them as comma-separated values in the correct order

PARAMETER FORMAT EXAMPLES:
- Single integer: 935
- Single string: customer_data
- Multiple parameters: 935, ["field1", "field2"]
- String and integer: database_name, 60

Your role is to execute workflow steps by:
1. Understanding the objective of each step
2. Choosing the most appropriate tool(s) to achieve the objective
3. Using tools with CORRECT parameter formats (no JSON wrapping)
4. Evaluating tool results against success/failure criteria
5. Making decisions about next steps based on outcomes

Use the following format:

Question: the workflow step you need to execute
Thought: analyze the objective and determine what needs to be done
Action: choose the most appropriate action from [{tool_names}]
Action Input: provide the parameter values in the correct format (NOT JSON)
Observation: analyze the result of the action
... (repeat Thought/Action/Action Input/Observation as needed)
Thought: evaluate if the step objective has been achieved based on success/failure criteria
Final Answer: provide a clear assessment of whether the step succeeded or failed, with reasoning

CRITICAL RULES:
1. If no suitable tool exists for an objective, respond with "NO_SUITABLE_TOOL"
2. If a tool fails or returns unsatisfactory results, try alternative approaches if available
3. Always evaluate results against the provided success/failure criteria
4. If you cannot achieve the objective after trying available tools, respond with "OBJECTIVE_FAILED"
5. NEVER wrap Action Input in JSON format - use direct parameter values

Begin!

Question: {input}
Thought:{agent_scratchpad}"""

        self.react_prompt = PromptTemplate.from_template(react_template)
        
        # Create enhanced ReAct agent
        self.react_agent = create_react_agent(
            llm=self.llm,
            tools=self.available_tools,
            prompt=self.react_prompt
        )
        
        # Create agent executor with better error handling
        self.agent_executor = AgentExecutor(
            agent=self.react_agent,
            tools=self.available_tools,
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=15,  # Increased for more thorough reasoning
            early_stopping_method="generate",
            return_intermediate_steps=True  # Important for debugging
        )
        
    def execute_step(self, state: AgentState) -> AgentState:
        """
        Enhanced step execution using ReAct pattern for intelligent decision making.
        """
        if not state["execution_queue"]:
            state["execution_status"] = "completed"
            return state
            
        current_step_id = state["execution_queue"][0]
        state["current_step_id"] = current_step_id
        
        print(f"🔧 Execution Agent: Executing step '{current_step_id}'")
        
        # Handle END step
        if current_step_id.upper() == "END":
            state["execution_status"] = "completed"
            state["execution_queue"] = []
            print("✅ Execution Agent: Workflow completed successfully")
            return state
        
        # Get current step details
        if not state["playbook"] or current_step_id not in state["playbook"]["steps"]:
            if current_step_id.lower() in ['end', 'stop', 'finish', 'complete']:
                state["execution_status"] = "completed"
                state["execution_queue"] = []
                print("✅ Execution Agent: Workflow completed successfully")
                return state
            else:
                state["execution_status"] = "failed"
                state["final_output"]["error"] = f"Step '{current_step_id}' not found in playbook"
                return state
            
        current_step = state["playbook"]["steps"][current_step_id]
        
        try:
            # Execute the step using ReAct pattern
            if current_step["action"] == "achieve_objective":
                result = self._execute_objective_step(current_step, state)
            elif current_step["action"] == "evaluate_condition":
                result = self._evaluate_condition_step(current_step, state)
            elif current_step["action"] == "notify":
                result = self._notify_step(current_step, state)
            else:
                result = {"status": "success", "message": f"Executed step: {current_step['name']}"}
            
            # Check for critical failures
            if result.get("critical_failure"):
                print(f"🛑 Critical failure in step '{current_step_id}': {result.get('message')}")
                state["execution_status"] = "failed"
                state["final_output"]["error"] = result.get("message", "Critical failure occurred")
                return state
            
            # Log the execution
            log_entry = {
                "step_id": current_step_id,
                "step_name": current_step.get("name", ""),
                "objective": current_step.get("objective", ""),
                "result": result,
                "timestamp": "now"
            }
            
            if "execution_log" not in state:
                state["execution_log"] = []
            state["execution_log"].append(log_entry)
            
            # Update context data
            if result.get("context_data"):
                state["context_data"].update(result["context_data"])
            
            # Store last tool result
            state["last_tool_result"] = result
            
            # Determine next step using enhanced logic
            next_step_id = self._determine_next_step(current_step, result)
            
            # Update execution queue
            state["execution_queue"] = state["execution_queue"][1:]
            
            if next_step_id and next_step_id != "END":
                state["execution_queue"].insert(0, next_step_id)
            
            print(f"✅ Step '{current_step_id}' completed. Next: '{next_step_id}'")
            
        except Exception as e:
            print(f"❌ Execution Agent Error in step '{current_step_id}': {str(e)}")
            state["execution_status"] = "failed"
            state["final_output"]["error"] = f"Execution failed at step '{current_step_id}': {str(e)}"
            
        return state
    
    def _execute_objective_step(self, step: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """
        Execute an objective-based step using ReAct agent for intelligent tool selection.
        """
        objective = step.get("objective", "")
        success_criteria = step.get("success_criteria", "")
        failure_criteria = step.get("failure_criteria", "")
        context_requirements = step.get("context_requirements", [])
        step_description = step.get("description", "")
        
        # Prepare context information including error code
        available_context = {"error_code": state["error_code"]}
        for req in context_requirements:
            if req in state["context_data"]:
                available_context[req] = state["context_data"][req]
        
        # Create detailed tool parameter guidance
        tool_param_guidance = []
        for tool in self.available_tools:
            tool_schema = getattr(tool, 'args_schema', None)
            if tool_schema:
                schema_info = tool_schema.schema()
                params = []
                for prop_name, prop_info in schema_info.get('properties', {}).items():
                    param_type = prop_info.get('type', 'unknown')
                    param_desc = prop_info.get('description', '')
                    params.append(f"      {prop_name}: {param_type} - {param_desc}")
                param_str = "\n".join(params) if params else "      No parameters"
                tool_param_guidance.append(f"    {tool.name}:\n{param_str}")
        
        # Create comprehensive question for ReAct agent
        question = f"""
        WORKFLOW STEP EXECUTION:
        
        Step: {step.get('name', 'Unnamed Step')}
        Objective: {objective}
        Description: {step_description}
        
        SUCCESS CRITERIA: {success_criteria}
        FAILURE CRITERIA: {failure_criteria}
        
        Available Context:
        {json.dumps(available_context, indent=2) if available_context else "No specific context required"}
        
        Previous Step Result:
        {json.dumps(state.get('last_tool_result', {}), indent=2)}
        
        TOOL PARAMETER REQUIREMENTS:
        {chr(10).join(tool_param_guidance)}
        
        CRITICAL PARAMETER RULES:
        - For integer parameters like 'errorcode': Use plain integers (e.g., 935)
        - For list parameters like 'required_fields': Use Python list format (e.g., ["field1", "field2"])
        - For string parameters: Use plain strings without extra quotes
        - NEVER wrap parameters in JSON format
        - Error code from context is: {state["error_code"]}
        
        Your task is to achieve the objective using the available tools with CORRECT parameter formats.
        Evaluate your results against the success/failure criteria.
        If you cannot find suitable tools or achieve the objective, clearly state this in your Final Answer.
        """
        
        try:
            print(f"🤖 ReAct Agent executing objective: {objective}")
            
            # Use ReAct agent to execute the step
            result = self.agent_executor.invoke({"input": question})
            agent_output = result.get('output', '') if hasattr(result, 'get') else str(result)
            
            print(f"🔍 ReAct Agent Output: {agent_output}")
            
            # Enhanced result analysis
            result_analysis = self._analyze_agent_result(agent_output, success_criteria, failure_criteria)
            
            # Check for critical conditions
            if "NO_SUITABLE_TOOL" in agent_output:
                return {
                    "status": "failure",
                    "critical_failure": True,
                    "message": f"No suitable tool available for objective: {objective}",
                    "agent_output": agent_output,
                    "reason": "tool_unavailable"
                }
            
            if "OBJECTIVE_FAILED" in agent_output:
                return {
                    "status": "failure",
                    "critical_failure": True,
                    "message": f"Failed to achieve objective: {objective}",
                    "agent_output": agent_output,
                    "reason": "objective_not_achieved"
                }
            
            # Extract tool results from intermediate steps
            tool_results = []
            context_data = {}
            
            if hasattr(result, 'get') and 'intermediate_steps' in result:
                for step_result in result['intermediate_steps']:
                    if len(step_result) > 1:
                        tool_name = step_result[0].tool if hasattr(step_result[0], 'tool') else 'unknown'
                        tool_output = step_result[1]
                        tool_results.append({
                            "tool": tool_name,
                            "output": tool_output
                        })
                        # Extract potential context data
                        context_data[f"{tool_name}_result"] = tool_output
            
            return {
                "status": result_analysis["status"],
                "objective": objective,
                "success_criteria": success_criteria,
                "failure_criteria": failure_criteria,
                "tool_results": tool_results,
                "agent_output": agent_output,
                "analysis": result_analysis,
                "context_data": context_data,
                "message": result_analysis["message"]
            }
            
        except Exception as e:
            print(f"❌ Error in ReAct execution: {str(e)}")
            return {
                "status": "failure",
                "critical_failure": True,
                "objective": objective,
                "error": str(e),
                "message": f"ReAct execution failed: {str(e)}"
            }
    
    def _analyze_agent_result(self, agent_output: str, success_criteria: str, failure_criteria: str) -> Dict[str, Any]:
        """
        Analyze the ReAct agent output to determine success/failure.
        """
        output_lower = agent_output.lower()
        
        # Enhanced success/failure detection
        success_indicators = ['success', 'completed', 'achieved', 'valid', 'connected', 'verified', 'passed', 'created', 'saved']
        failure_indicators = ['failed', 'error', 'invalid', 'disconnected', 'rejected', 'unable', 'cannot', 'validation error']
        
        # Check for explicit success/failure in output
        success_count = sum(1 for indicator in success_indicators if indicator in output_lower)
        failure_count = sum(1 for indicator in failure_indicators if indicator in output_lower)
        
        # Use LLM for more sophisticated analysis if indicators are ambiguous
        if abs(success_count - failure_count) <= 1:  # Ambiguous result
            try:
                analysis_prompt = f"""
                Analyze the following agent output and determine if it indicates success or failure:
                
                Agent Output: {agent_output}
                Success Criteria: {success_criteria}
                Failure Criteria: {failure_criteria}
                
                Respond with exactly: SUCCESS or FAILURE followed by a brief explanation.
                """
                
                analysis_result = self.llm.invoke([HumanMessage(content=analysis_prompt)])
                analysis_text = analysis_result.content.upper()
                
                if "SUCCESS" in analysis_text:
                    return {
                        "status": "success",
                        "message": "Objective achieved based on LLM analysis",
                        "llm_analysis": analysis_result.content
                    }
                else:
                    return {
                        "status": "failure", 
                        "message": "Objective not achieved based on LLM analysis",
                        "llm_analysis": analysis_result.content
                    }
            except:
                pass  # Fall back to simple analysis
        
        # Simple indicator-based analysis
        if success_count > failure_count:
            return {
                "status": "success",
                "message": f"Objective achieved (success indicators: {success_count})",
                "confidence": "indicator_based"
            }
        else:
            return {
                "status": "failure",
                "message": f"Objective not achieved (failure indicators: {failure_count})",
                "confidence": "indicator_based"
            }
    
    def _evaluate_condition_step(self, step: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """Enhanced condition evaluation using ReAct reasoning."""
        objective = step.get("objective", "")
        last_result = state.get("last_tool_result", {})
        context_data = state.get("context_data", {})
        
        evaluation_question = f"""
        CONDITION EVALUATION TASK:
        
        Objective: {objective}
        
        Previous Step Result: {json.dumps(last_result, indent=2)}
        Available Context: {json.dumps(context_data, indent=2)}
        
        Your task is to evaluate whether the condition/objective is met based on the available information.
        Use the available tools if you need to gather additional information.
        
        Provide a clear TRUE or FALSE determination with reasoning.
        """
        
        try:
            result = self.agent_executor.invoke({"input": evaluation_question})
            agent_output = result.get('output', '') if hasattr(result, 'get') else str(result)
            
            # Determine condition result
            condition_met = self._determine_condition_result(agent_output)
            
            return {
                "status": "success" if condition_met else "failure",
                "objective": objective,
                "condition_met": condition_met,
                "agent_output": agent_output,
                "message": f"Condition evaluation: {condition_met}"
            }
            
        except Exception as e:
            return {
                "status": "failure",
                "critical_failure": True,
                "objective": objective,
                "error": str(e),
                "message": f"Condition evaluation failed: {str(e)}"
            }
    
    def _determine_condition_result(self, agent_output: str) -> bool:
        """Determine if condition is met from agent output."""
        output_lower = agent_output.lower()
        
        # Look for explicit true/false statements
        if "true" in output_lower and "false" not in output_lower:
            return True
        if "false" in output_lower and "true" not in output_lower:
            return False
        
        # Look for positive/negative indicators
        positive_indicators = ['yes', 'confirmed', 'met', 'satisfied', 'passed', 'valid']
        negative_indicators = ['no', 'denied', 'not met', 'failed', 'invalid', 'unsatisfied']
        
        positive_count = sum(1 for indicator in positive_indicators if indicator in output_lower)
        negative_count = sum(1 for indicator in negative_indicators if indicator in output_lower)
        
        return positive_count > negative_count
    
    def _notify_step(self, step: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """Enhanced notification step."""
        objective = step.get("objective", "")
        description = step.get("description", "")
        
        print(f"📢 Notification: {objective}")
        print(f"   Details: {description}")
        
        return {
            "status": "success",
            "objective": objective,
            "message": f"Notification completed: {objective}"
        }
    
    def _determine_next_step(self, current_step: Dict[str, Any], result: Dict[str, Any]) -> str:
        """Enhanced next step determination."""
        next_steps = current_step.get("next_steps", {})
        result_status = result.get("status", "unknown")
        
        # Check for critical failure - should end workflow
        if result.get("critical_failure"):
            return "END"
        
        # Try to find next step based on result status
        next_step = None
        if result_status in next_steps:
            next_step = next_steps[result_status]
        elif "default" in next_steps:
            next_step = next_steps["default"]
        else:
            next_step = "END"
        
        # Normalize end step variations
        if next_step and next_step.lower() in ['end', 'stop', 'finish', 'complete']:
            next_step = "END"
            
        return next_step



def create_multi_agent_workflow(llm: ChatOpenAI, available_tools: List[BaseTool]) -> StateGraph:
    """
    Creates the enhanced multi-agent workflow with ReAct-powered execution.
    """
    planning_agent = PlanningAgent(llm, available_tools)
    execution_agent = ExecutionAgent(llm, available_tools)
    
    # Create the state graph
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("planning", planning_agent.analyze_sop)
    workflow.add_node("execution", execution_agent.execute_step)
    
    # Set entry point
    workflow.set_entry_point("planning")
    
    # Enhanced routing logic
    def route_after_planning(state: AgentState) -> Literal["execution", "__end__"]:
        """Route after planning phase."""
        if state["execution_status"] == "ready_to_execute" and state["execution_queue"]:
            return "execution"
        else:
            return "__end__"
    
    def route_after_execution(state: AgentState) -> Literal["execution", "__end__"]:
        """Route after execution phase."""
        status = state["execution_status"]
        
        if status == "completed":
            print("🎉 Workflow completed successfully!")
            return "__end__"
        elif status == "failed":
            print("💥 Workflow failed - terminating")
            return "__end__"
        elif state["execution_queue"]:
            return "execution"
        else:
            return "__end__"
    
    # Add conditional edges
    workflow.add_conditional_edges(
        "planning",
        route_after_planning,
        {
            "execution": "execution",
            "__end__": END
        }
    )
    
    workflow.add_conditional_edges(
        "execution",
        route_after_execution,
        {
            "execution": "execution",
            "__end__": END
        }
    )
    
    return workflow


# Example usage and testing
if __name__ == "__main__":
    from langchain_core.tools import tool
    
    # Enhanced mock tools for demonstration
    @tool
    def validate_data(data_source: str) -> str:
        """Validates data from the specified source."""
        # Simulate potential failure
        if "invalid" in data_source.lower():
            return f"Data validation FAILED for {data_source} - Status: Invalid data format"
        return f"Data validation COMPLETED for {data_source} - Status: Valid, 1000 records processed"
    
    @tool
    def check_database_connection(database_name: str) -> str:
        """Checks connection to the specified database."""
        # Simulate connection scenarios
        if "test" in database_name.lower():
            return f"Database connection FAILED for {database_name} - Status: Connection timeout"
        return f"Database connection VERIFIED for {database_name} - Status: Connected, latency 45ms"
    
    @tool
    def execute_query(query: str) -> str:
        """Executes a database query."""
        if len(query) < 10:
            return f"Query execution FAILED: {query} - Status: Invalid query format"
        return f"Query executed SUCCESSFULLY: {query} - Rows affected: 150, execution time: 0.3s"
    
    @tool
    def send_notification(message: str, recipient: str) -> str:
        """Sends a notification message."""
        return f"Notification DELIVERED to {recipient}: {message} - Status: Sent via email"
    
    @tool 
    def repair_data(data_source: str, repair_type: str) -> str:
        """Repairs data issues in the specified source."""
        return f"Data repair COMPLETED for {data_source} using {repair_type} - Status: 45 records fixed"
    @tool
    def report_file_creator(errorcode: int, required_fields: list[str]) -> str:
        """
        Creates a report file for a given error code with specified fields.
        Args:
            errorcode (int): The error code for the report.
            required_fields (list[str]): A list of fields to include in the report.
        Returns:
            str: A confirmation message indicating the report file was created.
        """
        # ---- rest of the code -----
        print(f"Tool 'report_file_creator' called with errorcode: {errorcode} and fields: {required_fields}")
        return f"Report file created for error {errorcode}."

    @tool
    def store_in_audit_db(errorcode: int) -> str:
        """
        Stores a record in the audit table based on the provided error code.
        Args:
            errorcode (int): The error code associated with the record to be stored (e.g., 123).
        Returns:
            str: A confirmation message indicating that the data has been successfully saved to the audit table.
        """
        print(f"tool audit called with error code: {errorcode}")
        return "Data saved"
    # Initialize LLM (replace with your preferred LLM)
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    
    # Available tools
    tools = [validate_data, check_database_connection, execute_query, send_notification, repair_data, store_in_audit_db, report_file_creator]

    # Create the enhanced multi-agent workflow
    workflow = create_multi_agent_workflow(llm, tools)
    
    # Compile the graph
    app = workflow.compile()
    
    # Enhanced example SOP content with more complex logic
    sample_sop = """
    SOP for Error Code 935 - Data Validation Failure
    
    OBJECTIVE: Resolve data validation issues and ensure system integrity
    
    WORKFLOW:
    1. VALIDATE DATA SOURCE: Check the integrity of 'customer_data' source
       - IF validation passes: Proceed to step 6 (success notification)
       - IF validation fails: Continue to step 2
    
    2. DIAGNOSE CONNECTION: Verify database connectivity to 'main_db'
       - IF connection is healthy: Proceed to step 3
       - IF connection fails: Send error notification and END
    
    3. REPAIR DATA: Execute data repair using 'cleanup' method on 'customer_data'
       - IF repair succeeds: Proceed to step 4
       - IF repair fails: Send error notification and END
    
    4. RE-VALIDATE: Re-check the 'customer_data' after repair
       - IF validation now passes: Proceed to step 5
       - IF validation still fails: Send failure notification and END
    
    5. EXECUTE MAINTENANCE: Run maintenance query 'UPDATE customers SET status = active WHERE status IS NULL'
       - IF query succeeds: Proceed to step 6
       - IF query fails: Send error notification and END
    
    6. SUCCESS NOTIFICATION: Send success notification to 'admin@company.com'
       - Always END after this step
    
    ERROR HANDLING: For any unexpected errors, send detailed error notification to 'admin@company.com' and END
    """
    
    # Test the enhanced multi-agent system
    initial_state: AgentState = {
        "error_code": 935,
        "sop_content": sample_sop,
        "playbook": None,
        "execution_queue": [],
        "data": None,
        "last_tool_result": None,
        "final_output": {},
        "current_step_id": None,
        "execution_status": "planning",
        "execution_log": [],
        "context_data": {}
    }
    
    # Run the enhanced multi-agent workflow
    try:
        print("🚀 Starting Enhanced Multi-Agent Workflow with ReAct Pattern...")
        print("=" * 70)
        
        result = app.invoke(initial_state)
        
        print("=" * 70)
        print("📊 FINAL WORKFLOW RESULTS:")
        print(f"Execution Status: {result['execution_status']}")
        print(f"Steps Executed: {len(result.get('execution_log', []))}")
        
        if result.get('execution_log'):
            print("\n📋 DETAILED EXECUTION LOG:")
            for i, log_entry in enumerate(result['execution_log'], 1):
                print(f"\n{i}. STEP: {log_entry['step_name']}")
                print(f"   Objective: {log_entry.get('objective', 'N/A')}")
                print(f"   Result: {log_entry['result'].get('message', 'No message')}")
                print(f"   Status: {log_entry['result'].get('status', 'Unknown')}")
                
                # Show tool results if available
                if log_entry['result'].get('tool_results'):
                    print(f"   Tools Used:")
                    for tool_result in log_entry['result']['tool_results']:
                        print(f"     - {tool_result.get('tool', 'Unknown')}: {tool_result.get('output', 'No output')}")
        
        if result.get('context_data'):
            print(f"\n🔍 ACCUMULATED CONTEXT DATA:")
            for key, value in result['context_data'].items():
                print(f"   {key}: {value}")
        
        if result.get('final_output'):
            print(f"\n🎯 FINAL OUTPUT:")
            for key, value in result['final_output'].items():
                print(f"   {key}: {value}")
                
        # Show playbook structure
        if result.get('playbook'):
            print(f"\n📖 GENERATED PLAYBOOK STRUCTURE:")
            print(f"   Name: {result['playbook'].get('name', 'Unknown')}")
            print(f"   Total Steps: {len(result['playbook'].get('steps', {}))}")
            print(f"   Start Step: {result['playbook'].get('start_step', 'Unknown')}")
            
            print(f"\n   STEP BREAKDOWN:")
            for step_id, step_data in result['playbook'].get('steps', {}).items():
                print(f"     {step_id}: {step_data.get('name', 'Unnamed')} -> {step_data.get('next_steps', {})}")
    
    except Exception as e:
        print(f"❌ Error running enhanced multi-agent workflow: {str(e)}")
        import traceback
        traceback.print_exc()