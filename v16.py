from typing import TypedDict, List, Optional, Any, Dict, Literal
from langgraph import StateGraph, END
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import BaseTool
from langchain_openai import ChatOpenAI
from langchain.agents import create_react_agent, AgentExecutor
from langchain_core.prompts import PromptTemplate
import json
import re
from pydantic import BaseModel, Field

# Agent State Definition
class AgentState(TypedDict):
    """
    Defines the shared state for the playbook-driven multi-agent system.
    """
    error_code: int
    sop_content: str
    
    playbook: Optional[Dict[str, Any]]
    """The structured JSON workflow graph generated by the Planning Agent."""

    execution_queue: List[str]
    """A queue of node IDs from the playbook that the Execution Agent needs to process."""
    
    data: Optional[Any]
    """Holds operational data, like the pandas DataFrame for error 935."""

    last_tool_result: Optional[Dict[str, Any]]
    """Stores the output from the last executed tool for conditional evaluation."""

    final_output: Dict[str, Any]
    """A dictionary to accumulate final results and statuses from the workflow."""

    current_step_id: Optional[str]
    """Currently executing step ID."""

    execution_status: str
    """Status of execution: 'planning', 'executing', 'completed', 'failed'."""

    execution_log: List[Dict[str, Any]]
    """Log of all executed steps and their results."""

    context_data: Dict[str, Any]
    """Accumulated context data from all executed steps."""


# Playbook Schema for structured output
class PlaybookStep(BaseModel):
    """Represents a single step in the playbook"""
    id: str = Field(description="Unique identifier for the step")
    name: str = Field(description="Human-readable name of the step")
    action: str = Field(description="The action to be performed")
    objective: str = Field(description="What the step is trying to achieve")
    success_criteria: str = Field(description="How to determine if the step was successful")
    failure_criteria: str = Field(description="How to determine if the step failed")
    next_steps: Dict[str, str] = Field(description="Next steps based on conditions (success, failure, etc.)")
    description: str = Field(description="Detailed description of what this step does")
    context_requirements: Optional[List[str]] = Field(description="What context/data is needed for this step")


class PlanningAgent:
    """
    Enhanced Planning Agent that creates ReAct-friendly playbooks.
    """
    
    def __init__(self, llm: ChatOpenAI, available_tools: List[BaseTool]):
        self.llm = llm
        self.available_tools = available_tools
        self.tool_names = [tool.name for tool in available_tools]
        
    def analyze_sop(self, state: AgentState) -> AgentState:
        """
        Analyzes the SOP content and creates a structured playbook optimized for ReAct execution.
        """
        print("üîç Planning Agent: Starting SOP analysis...")
        
        sop_content = state["sop_content"]
        error_code = state["error_code"]
        
        # Update execution status
        state["execution_status"] = "planning"
        state["context_data"] = {}
        
        # Create detailed tool descriptions
        tool_descriptions = []
        for tool in self.available_tools:
            tool_descriptions.append(f"- {tool.name}: {tool.description}")
        
        # Enhanced system prompt for ReAct-optimized planning
        system_prompt = f"""
        You are an expert SOP analyzer that creates ReAct-optimized playbooks for intelligent execution agents.
        
        Your task is to analyze the SOP and create a JSON playbook where each step provides enough context 
        for a ReAct agent to intelligently choose tools and make decisions based on tool outcomes.
        
        AVAILABLE TOOLS:
        {chr(10).join(tool_descriptions)}
        
        CRITICAL DESIGN PRINCIPLES:
        1. Each step should define OBJECTIVES, not specific tools to use
        2. Include clear SUCCESS and FAILURE criteria that a ReAct agent can evaluate
        3. Steps should be tool-agnostic - let the ReAct agent choose the best tool for the objective
        4. Provide rich context about what each step is trying to achieve
        5. Design conditional flows that can handle multiple scenarios
        
        Guidelines for ReAct-optimized playbooks:
        1. Focus on WHAT needs to be achieved, not HOW to achieve it
        2. Provide clear success/failure criteria for each step
        3. Include context requirements (what data/information is needed)
        4. Design steps that allow the ReAct agent to reason about tool selection
        5. Create robust error handling and alternative paths
        6. Each step should be self-contained with clear objectives
        
        Error Code Context: {error_code}
        
        IMPORTANT: Return ONLY the JSON playbook, no other text or explanations.
        """
        
        human_prompt = f"""
        Create a ReAct-optimized JSON playbook for this SOP:
        
        {sop_content}
        
        Required JSON structure:
        {{
            "name": "Playbook name",
            "description": "What this playbook accomplishes",
            "start_step": "first_step_id",
            "steps": {{
                "step_id": {{
                    "id": "step_id",
                    "name": "Step name",
                    "action": "achieve_objective|evaluate_condition|notify|end",
                    "objective": "What this step needs to accomplish",
                    "success_criteria": "How to determine success",
                    "failure_criteria": "How to determine failure", 
                    "next_steps": {{"success": "next_id", "failure": "error_id", "default": "END"}},
                    "description": "Detailed description",
                    "context_requirements": ["required_data1", "required_data2"]
                }}
            }}
        }}
        
        RULES:
        1. Use "END" for terminal steps
        2. Focus on objectives, not specific tools
        3. Include rich success/failure criteria
        4. All referenced step IDs must exist
        5. Make it ReAct agent friendly - provide reasoning context
        """
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt)
        ]
        
        try:
            response = self.llv.invoke(messages)
            playbook_json = self._extract_json_from_response(response.content)
            playbook = self._validate_and_structure_playbook(playbook_json)
            
            state["playbook"] = playbook
            state["execution_queue"] = [playbook["start_step"]] if playbook else []
            state["execution_status"] = "ready_to_execute"
            
            print(f"‚úÖ Planning Agent: Created ReAct-optimized playbook with {len(playbook.get('steps', {}))} steps")
            print(f"üìã Start step: {playbook['start_step']}")
            
        except Exception as e:
            print(f"‚ùå Planning Agent Error: {str(e)}")
            state["playbook"] = None
            state["execution_queue"] = []
            state["execution_status"] = "failed"
            state["final_output"]["error"] = f"Planning failed: {str(e)}"
        
        return state
    
    def _extract_json_from_response(self, response_content: str) -> Dict[str, Any]:
        """Extracts JSON from the LLM response."""
        json_pattern = r'```json\s*(.*?)\s*```'
        json_match = re.search(json_pattern, response_content, re.DOTALL)
        
        if json_match:
            json_str = json_match.group(1)
        else:
            json_str = response_content.strip()
        
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            json_start = response_content.find('{')
            json_end = response_content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_str = response_content[json_start:json_end]
                return json.loads(json_str)
            else:
                raise ValueError(f"Could not extract valid JSON from response: {str(e)}")
    
    def _validate_and_structure_playbook(self, playbook_json: Dict[str, Any]) -> Dict[str, Any]:
        """Validates the playbook structure."""
        if "steps" not in playbook_json:
            raise ValueError("Playbook must contain 'steps' field")
        
        if "start_step" not in playbook_json:
            playbook_json["start_step"] = list(playbook_json["steps"].keys())[0]
        
        if playbook_json["start_step"] not in playbook_json["steps"]:
            raise ValueError(f"Start step '{playbook_json['start_step']}' not found in steps")
        
        # Ensure each step has required fields
        for step_id, step_data in playbook_json["steps"].items():
            if "id" not in step_data:
                step_data["id"] = step_id
            if "next_steps" not in step_data:
                step_data["next_steps"] = {"default": "END"}
            if "action" not in step_data:
                step_data["action"] = "achieve_objective"
            if "success_criteria" not in step_data:
                step_data["success_criteria"] = "Operation completed successfully"
            if "failure_criteria" not in step_data:
                step_data["failure_criteria"] = "Operation failed or error occurred"
            if "context_requirements" not in step_data:
                step_data["context_requirements"] = []
                
            # Normalize end step references
            if "next_steps" in step_data:
                normalized_next_steps = {}
                for condition, next_step in step_data["next_steps"].items():
                    if isinstance(next_step, str) and next_step.lower() in ['end', 'stop', 'finish', 'complete']:
                        normalized_next_steps[condition] = "END"
                    else:
                        normalized_next_steps[condition] = next_step
                step_data["next_steps"] = normalized_next_steps
        
        return playbook_json




class ExecutionAgent:
    """
    Enhanced Execution Agent that uses ReAct pattern for intelligent tool selection and decision making.
    """
    
    def __init__(self, llm: ChatOpenAI, available_tools: List[BaseTool]):
        self.llm = llm
        self.available_tools = available_tools
        self.tool_dict = {tool.name: tool for tool in available_tools}
        
        # Enhanced ReAct prompt for workflow execution with better tool input formatting
        react_template = """You are an intelligent workflow execution agent. You have access to the following tools:

{tools}

Your role is to execute workflow steps by:
1. Understanding the objective of each step
2. Choosing the most appropriate tool(s) to achieve the objective
3. Evaluating tool results against success/failure criteria
4. Making decisions about next steps based on outcomes

CRITICAL INPUT FORMAT RULES:
- For tools expecting integers: use plain numbers without quotes (e.g., 935, not "935")
- For tools expecting lists: use proper list format (e.g., ["item1", "item2"])
- For tools expecting strings: use plain text without JSON formatting
- Do NOT wrap simple values in JSON objects unless the tool specifically expects a JSON object
- Match the exact parameter names defined in the tool signature

Use the following format:

Question: the workflow step you need to execute
Thought: analyze the objective and determine what needs to be done
Action: choose the most appropriate action from [{tool_names}]
Action Input: provide the input in the EXACT format expected by the tool (check tool signature carefully)
Observation: analyze the result of the action
... (repeat Thought/Action/Action Input/Observation as needed)
Thought: evaluate if the step objective has been achieved based on success/failure criteria
Final Answer: provide a clear assessment of whether the step succeeded or failed, with reasoning

EXAMPLES OF CORRECT ACTION INPUT FORMATS:
- For report_file_creator(errorcode: int, required_fields: list[str]):
  Action Input: errorcode=935, required_fields=["claim_id", "status", "timestamp"]
- For store_in_audit_db(errorcode: int):
  Action Input: errorcode=935
- For validate_data(data_source: str):
  Action Input: data_source=customer_data

CRITICAL RULES:
1. If no suitable tool exists for an objective, respond with "NO_SUITABLE_TOOL"
2. If a tool fails or returns unsatisfactory results, try alternative approaches if available
3. Always evaluate results against the provided success/failure criteria
4. If you cannot achieve the objective after trying available tools, respond with "OBJECTIVE_FAILED"
5. Pay close attention to tool parameter types and format inputs accordingly

Begin!

Question: {input}
Thought:{agent_scratchpad}"""

        self.react_prompt = PromptTemplate.from_template(react_template)
        
        # Create enhanced ReAct agent with custom tool wrapper
        self.wrapped_tools = [self._wrap_tool_with_input_parser(tool) for tool in available_tools]
        
        self.react_agent = create_react_agent(
            llm=self.llm,
            tools=self.wrapped_tools,
            prompt=self.react_prompt
        )
        # Inside __init__ method:
        tool_args_info = []
        for tool in self.available_tools:
            schema = tool.args_schema.schema() if tool.args_schema else {}
            args = schema.get("properties", {})
            arg_desc = ", ".join([f"{k}: {v.get('type', 'string')}" for k,v in args.items()])
            tool_args_info.append(f"{tool.name}: {arg_desc}")

        self.react_prompt = self.react_prompt.partial(
            tool_args="\n".join(tool_args_info)
        )
        
        # Create agent executor with better error handling
        self.agent_executor = AgentExecutor(
            agent=self.react_agent,
            tools=self.wrapped_tools,
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=10,
            early_stopping_method="generate",
            return_intermediate_steps=True
        )
    
    def _wrap_tool_with_input_parser(self, tool: BaseTool) -> BaseTool:
        """
        Wraps a tool with enhanced input parsing to handle ReAct agent output better.
        """
        from langchain_core.tools import StructuredTool
        import inspect
        
        original_func = tool.func if hasattr(tool, 'func') else None
        if not original_func:
            return tool
        
        # Get the original function signature
        sig = inspect.signature(original_func)
        
        def enhanced_tool_wrapper(*args, **kwargs):
            """Enhanced wrapper that parses inputs more intelligently."""
            try:
                # If called with a single string argument that looks like structured data
                if len(args) == 1 and len(kwargs) == 0 and isinstance(args[0], str):
                    input_str = args[0].strip()
                    
                    # Try to parse as key=value pairs
                    if '=' in input_str and not input_str.startswith('{'):
                        parsed_kwargs = self._parse_key_value_string(input_str, sig)
                        if parsed_kwargs:
                            return original_func(**parsed_kwargs)
                    
                    # Try to parse as JSON
                    elif input_str.startswith('{') and input_str.endswith('}'):
                        try:
                            import json
                            parsed_data = json.loads(input_str)
                            if isinstance(parsed_data, dict):
                                # Convert the parsed data to match parameter types
                                converted_kwargs = self._convert_parameters(parsed_data, sig)
                                return original_func(**converted_kwargs)
                        except (json.JSONDecodeError, ValueError):
                            pass
                    
                    # For single parameter functions, try direct conversion
                    param_names = list(sig.parameters.keys())
                    if len(param_names) == 1:
                        param_name = param_names[0]
                        param_type = sig.parameters[param_name].annotation
                        
                        try:
                            converted_value = self._convert_single_value(input_str, param_type)
                            return original_func(**{param_name: converted_value})
                        except (ValueError, TypeError):
                            pass
                
                # Convert kwargs to proper types
                if kwargs:
                    converted_kwargs = self._convert_parameters(kwargs, sig)
                    return original_func(*args, **converted_kwargs)
                
                # Fallback to original call
                return original_func(*args, **kwargs)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Tool input parsing error for {tool.name}: {str(e)}")
                print(f"   Raw args: {args}")
                print(f"   Raw kwargs: {kwargs}")
                # Try the original function as fallback
                try:
                    return original_func(*args, **kwargs)
                except Exception as original_error:
                    raise Exception(f"Tool execution failed. Original error: {str(original_error)}. Parser error: {str(e)}")
        
        # Create new tool with the enhanced wrapper
        return StructuredTool.from_function(
            func=enhanced_tool_wrapper,
            name=tool.name,
            description=tool.description,
            args_schema=tool.args_schema if hasattr(tool, 'args_schema') else None
        )
    
    from langchain.callbacks.base import BaseCallbackHandler

    class _InputValidatorCallback(BaseCallbackHandler):
        def on_tool_start(self, serialized, input_str, **kwargs):
            tool_name = serialized["name"]
            try:
                validated = self._validate_tool_input(tool_name, input_str)
                return validated
            except ValueError as e:
                return str(e)  # Return error to agent

        @property
        def _input_validator_callback(self):
            return self._InputValidatorCallback()
        

    def _validate_tool_input(self, tool_name: str, input_str: str) -> dict:
        """Validate and parse tool input"""
        tool = self.tool_dict.get(tool_name)
        if not tool:
            raise ValueError(f"Tool {tool_name} not found")
        
        try:
            # Parse JSON input
            input_data = json.loads(input_str) if isinstance(input_str, str) else input_str
            
            # Validate against schema
            if tool.args_schema:
                validated = tool.args_schema(**input_data)
                return validated.dict()
            return input_data
        except Exception as e:
            raise ValueError(f"Invalid input for {tool_name}: {str(e)}")
    
    def _parse_key_value_string(self, input_str: str, signature) -> Dict[str, Any]:
        """
        Parse key=value formatted string into proper parameters.
        Example: "errorcode=935, required_fields=['claim_id', 'status']"
        """
        try:
            pairs = []
            current_pair = ""
            bracket_count = 0
            quote_count = 0
            
            for char in input_str:
                if char in '[{':
                    bracket_count += 1
                elif char in ']}':
                    bracket_count -= 1
                elif char in '"\'':
                    quote_count = (quote_count + 1) % 2
                elif char == ',' and bracket_count == 0 and quote_count == 0:
                    pairs.append(current_pair.strip())
                    current_pair = ""
                    continue
                
                current_pair += char
            
            if current_pair.strip():
                pairs.append(current_pair.strip())
            
            result = {}
            for pair in pairs:
                if '=' not in pair:
                    continue
                    
                key, value = pair.split('=', 1)
                key = key.strip()
                value = value.strip()
                
                # Get expected type from signature
                if key in signature.parameters:
                    param_type = signature.parameters[key].annotation
                    converted_value = self._convert_single_value(value, param_type)
                    result[key] = converted_value
                else:
                    result[key] = value
            
            return result
            
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to parse key-value string: {input_str}. Error: {str(e)}")
            return {}
    
    def _convert_single_value(self, value_str: str, param_type):
        """Convert a string value to the expected parameter type."""
        import ast
        
        # Handle string types
        if param_type == str or param_type == 'str':
            return value_str.strip('\'"')
        
        # Handle integer types
        if param_type == int or param_type == 'int':
            # Remove any non-numeric characters except minus sign
            import re
            numeric_str = re.sub(r'[^\d-]', '', value_str)
            return int(numeric_str) if numeric_str else 0
        
        # Handle float types
        if param_type == float or param_type == 'float':
            import re
            numeric_str = re.sub(r'[^\d.-]', '', value_str)
            return float(numeric_str) if numeric_str else 0.0
        
        # Handle list types
        if hasattr(param_type, '__origin__') and param_type.__origin__ == list:
            try:
                # Try to parse as Python literal
                if value_str.startswith('[') and value_str.endswith(']'):
                    return ast.literal_eval(value_str)
                else:
                    # Split by comma and clean up
                    items = [item.strip().strip('\'"') for item in value_str.split(',')]
                    return [item for item in items if item]
            except:
                return [value_str]  # Fallback to single item list
        
        # Handle bool types
        if param_type == bool or param_type == 'bool':
            return value_str.lower() in ('true', '1', 'yes', 'on')
        
        # Try literal evaluation for complex types
        try:
            return ast.literal_eval(value_str)
        except:
            return value_str
    
    def _convert_parameters(self, params: Dict[str, Any], signature) -> Dict[str, Any]:
        """Convert parameter dictionary to proper types based on function signature."""
        converted = {}
        
        for key, value in params.items():
            if key in signature.parameters:
                param_type = signature.parameters[key].annotation
                
                # Skip conversion if already correct type
                if param_type != inspect.Parameter.empty and isinstance(value, param_type):
                    converted[key] = value
                    continue
                
                # Convert string values
                if isinstance(value, str):
                    converted[key] = self._convert_single_value(value, param_type)
                else:
                    converted[key] = value
            else:
                converted[key] = value
        
        return converted
    
    def execute_step(self, state: AgentState) -> AgentState:
        """
        Enhanced step execution using ReAct pattern for intelligent decision making.
        """
        if not state["execution_queue"]:
            state["execution_status"] = "completed"
            return state
            
        current_step_id = state["execution_queue"][0]
        state["current_step_id"] = current_step_id
        
        print(f"üîß Execution Agent: Executing step '{current_step_id}'")
        
        # Handle END step
        if current_step_id.upper() == "END":
            state["execution_status"] = "completed"
            state["execution_queue"] = []
            print("‚úÖ Execution Agent: Workflow completed successfully")
            return state
        
        # Get current step details
        if not state["playbook"] or current_step_id not in state["playbook"]["steps"]:
            if current_step_id.lower() in ['end', 'stop', 'finish', 'complete']:
                state["execution_status"] = "completed"
                state["execution_queue"] = []
                print("‚úÖ Execution Agent: Workflow completed successfully")
                return state
            else:
                state["execution_status"] = "failed"
                state["final_output"]["error"] = f"Step '{current_step_id}' not found in playbook"
                return state
            
        current_step = state["playbook"]["steps"][current_step_id]
        
        try:
            # Execute the step using ReAct pattern
            if current_step["action"] == "achieve_objective":
                result = self._execute_objective_step(current_step, state)
            elif current_step["action"] == "evaluate_condition":
                result = self._evaluate_condition_step(current_step, state)
            elif current_step["action"] == "notify":
                result = self._notify_step(current_step, state)
            else:
                result = {"status": "success", "message": f"Executed step: {current_step['name']}"}
            
            # Check for critical failures
            if result.get("critical_failure"):
                print(f"üõë Critical failure in step '{current_step_id}': {result.get('message')}")
                state["execution_status"] = "failed"
                state["final_output"]["error"] = result.get("message", "Critical failure occurred")
                return state
            
            # Log the execution
            log_entry = {
                "step_id": current_step_id,
                "step_name": current_step.get("name", ""),
                "objective": current_step.get("objective", ""),
                "result": result,
                "timestamp": "now"
            }
            
            if "execution_log" not in state:
                state["execution_log"] = []
            state["execution_log"].append(log_entry)
            
            # Update context data
            if result.get("context_data"):
                state["context_data"].update(result["context_data"])
            
            # Store last tool result
            state["last_tool_result"] = result
            
            # Determine next step using enhanced logic
            next_step_id = self._determine_next_step(current_step, result)
            
            # Update execution queue
            state["execution_queue"] = state["execution_queue"][1:]
            
            if next_step_id and next_step_id != "END":
                state["execution_queue"].insert(0, next_step_id)
            
            print(f"‚úÖ Step '{current_step_id}' completed. Next: '{next_step_id}'")
            
        except Exception as e:
            print(f"‚ùå Execution Agent Error in step '{current_step_id}': {str(e)}")
            state["execution_status"] = "failed"
            state["final_output"]["error"] = f"Execution failed at step '{current_step_id}': {str(e)}"
            
        return state
    
    # Add to ExecutionAgent class

    
    def _execute_objective_step(self, step: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """
        Execute an objective-based step using ReAct agent for intelligent tool selection.
        """
        objective = step.get("objective", "")
        success_criteria = step.get("success_criteria", "")
        failure_criteria = step.get("failure_criteria", "")
        context_requirements = step.get("context_requirements", [])
        step_description = step.get("description", "")
        
        # Prepare context information
        available_context = {}
        for req in context_requirements:
            if req in state["context_data"]:
                available_context[req] = state["context_data"][req]
        
        # Add error code to context if available
        if state.get("error_code"):
            available_context["error_code"] = state["error_code"]
        
        # Create comprehensive question for ReAct agent with enhanced tool usage instructions
        question = f"""
        WORKFLOW STEP EXECUTION:
        
        Step: {step.get('name', 'Unnamed Step')}
        Objective: {objective}
        Description: {step_description}
        
        SUCCESS CRITERIA: {success_criteria}
        FAILURE CRITERIA: {failure_criteria}
        
        Available Context:
        {json.dumps(available_context, indent=2) if available_context else "No specific context required"}
        
        Previous Step Result:
        {json.dumps(state.get('last_tool_result', {}), indent=2)}
        
        IMPORTANT TOOL USAGE INSTRUCTIONS:
        - When using tools that require integers (like error codes), provide plain numbers: errorcode=935
        - When using tools that require lists, use proper list format: required_fields=["field1", "field2"]
        - When using tools that require strings, provide plain text: data_source=customer_data
        - Do NOT wrap simple values in JSON objects unless specifically required
        - Check the tool signature carefully and match parameter names exactly
        
        Your task is to achieve the objective using the available tools. 
        Evaluate your results against the success/failure criteria.
        If you cannot find suitable tools or achieve the objective, clearly state this in your Final Answer.
        """
        
        try:
            print(f"ü§ñ ReAct Agent executing objective: {objective}")
            
            # Use ReAct agent to execute the step
            result = self.agent_executor.invoke({"input": question},config={"callbacks": [self._input_validator_callback])
            agent_output = result.get('output', '') if hasattr(result, 'get') else str(result)
            
            print(f"üîç ReAct Agent Output: {agent_output}")
            
            # Enhanced result analysis
            result_analysis = self._analyze_agent_result(agent_output, success_criteria, failure_criteria)
            
            # Check for critical conditions
            if "NO_SUITABLE_TOOL" in agent_output:
                return {
                    "status": "failure",
                    "critical_failure": True,
                    "message": f"No suitable tool available for objective: {objective}",
                    "agent_output": agent_output,
                    "reason": "tool_unavailable"
                }
            
            if "OBJECTIVE_FAILED" in agent_output:
                return {
                    "status": "failure",
                    "critical_failure": True,
                    "message": f"Failed to achieve objective: {objective}",
                    "agent_output": agent_output,
                    "reason": "objective_not_achieved"
                }
            
            # Extract tool results from intermediate steps
            tool_results = []
            context_data = {}
            
            if hasattr(result, 'get') and 'intermediate_steps' in result:
                for step_result in result['intermediate_steps']:
                    if len(step_result) > 1:
                        tool_name = step_result[0].tool if hasattr(step_result[0], 'tool') else 'unknown'
                        tool_output = step_result[1]
                        tool_results.append({
                            "tool": tool_name,
                            "output": tool_output
                        })
                        # Extract potential context data
                        context_data[f"{tool_name}_result"] = tool_output
            
            return {
                "status": result_analysis["status"],
                "objective": objective,
                "success_criteria": success_criteria,
                "failure_criteria": failure_criteria,
                "tool_results": tool_results,
                "agent_output": agent_output,
                "analysis": result_analysis,
                "context_data": context_data,
                "message": result_analysis["message"]
            }
            
        except Exception as e:
            print(f"‚ùå Error in ReAct execution: {str(e)}")
            return {
                "status": "failure",
                "critical_failure": True,
                "objective": objective,
                "error": str(e),
                "message": f"ReAct execution failed: {str(e)}"
            }
    
    def _analyze_agent_result(self, agent_output: str, success_criteria: str, failure_criteria: str) -> Dict[str, Any]:
        """
        Analyze the ReAct agent output to determine success/failure.
        """
        output_lower = agent_output.lower()
        
        # Enhanced success/failure detection
        success_indicators = ['success', 'completed', 'achieved', 'valid', 'connected', 'verified', 'passed']
        failure_indicators = ['failed', 'error', 'invalid', 'disconnected', 'rejected', 'unable', 'cannot']
        
        # Check for explicit success/failure in output
        success_count = sum(1 for indicator in success_indicators if indicator in output_lower)
        failure_count = sum(1 for indicator in failure_indicators if indicator in output_lower)
        
        # Use LLM for more sophisticated analysis if indicators are ambiguous
        if abs(success_count - failure_count) <= 1:  # Ambiguous result
            try:
                analysis_prompt = f"""
                Analyze the following agent output and determine if it indicates success or failure:
                
                Agent Output: {agent_output}
                Success Criteria: {success_criteria}
                Failure Criteria: {failure_criteria}
                
                Respond with exactly: SUCCESS or FAILURE followed by a brief explanation.
                """
                
                analysis_result = self.llm.invoke([HumanMessage(content=analysis_prompt)])
                analysis_text = analysis_result.content.upper()
                
                if "SUCCESS" in analysis_text:
                    return {
                        "status": "success",
                        "message": "Objective achieved based on LLM analysis",
                        "llm_analysis": analysis_result.content
                    }
                else:
                    return {
                        "status": "failure", 
                        "message": "Objective not achieved based on LLM analysis",
                        "llm_analysis": analysis_result.content
                    }
            except:
                pass  # Fall back to simple analysis
        
        # Simple indicator-based analysis
        if success_count > failure_count:
            return {
                "status": "success",
                "message": f"Objective achieved (success indicators: {success_count})",
                "confidence": "indicator_based"
            }
        else:
            return {
                "status": "failure",
                "message": f"Objective not achieved (failure indicators: {failure_count})",
                "confidence": "indicator_based"
            }
    
    def _evaluate_condition_step(self, step: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """Enhanced condition evaluation using ReAct reasoning."""
        objective = step.get("objective", "")
        last_result = state.get("last_tool_result", {})
        context_data = state.get("context_data", {})
        
        evaluation_question = f"""
        CONDITION EVALUATION TASK:
        
        Objective: {objective}
        
        Previous Step Result: {json.dumps(last_result, indent=2)}
        Available Context: {json.dumps(context_data, indent=2)}
        
        Your task is to evaluate whether the condition/objective is met based on the available information.
        Use the available tools if you need to gather additional information.
        
        Provide a clear TRUE or FALSE determination with reasoning.
        """
        
        try:
            result = self.agent_executor.invoke({"input": evaluation_question})
            agent_output = result.get('output', '') if hasattr(result, 'get') else str(result)
            
            # Determine condition result
            condition_met = self._determine_condition_result(agent_output)
            
            return {
                "status": "success" if condition_met else "failure",
                "objective": objective,
                "condition_met": condition_met,
                "agent_output": agent_output,
                "message": f"Condition evaluation: {condition_met}"
            }
            
        except Exception as e:
            return {
                "status": "failure",
                "critical_failure": True,
                "objective": objective,
                "error": str(e),
                "message": f"Condition evaluation failed: {str(e)}"
            }
    
    def _determine_condition_result(self, agent_output: str) -> bool:
        """Determine if condition is met from agent output."""
        output_lower = agent_output.lower()
        
        # Look for explicit true/false statements
        if "true" in output_lower and "false" not in output_lower:
            return True
        if "false" in output_lower and "true" not in output_lower:
            return False
        
        # Look for positive/negative indicators
        positive_indicators = ['yes', 'confirmed', 'met', 'satisfied', 'passed', 'valid']
        negative_indicators = ['no', 'denied', 'not met', 'failed', 'invalid', 'unsatisfied']
        
        positive_count = sum(1 for indicator in positive_indicators if indicator in output_lower)
        negative_count = sum(1 for indicator in negative_indicators if indicator in output_lower)
        
        return positive_count > negative_count
    
    def _notify_step(self, step: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """Enhanced notification step."""
        objective = step.get("objective", "")
        description = step.get("description", "")
        
        print(f"üì¢ Notification: {objective}")
        print(f"   Details: {description}")
        
        return {
            "status": "success",
            "objective": objective,
            "message": f"Notification completed: {objective}"
        }
    
    def _determine_next_step(self, current_step: Dict[str, Any], result: Dict[str, Any]) -> str:
        """Enhanced next step determination."""
        next_steps = current_step.get("next_steps", {})
        result_status = result.get("status", "unknown")
        
        # Check for critical failure - should end workflow
        if result.get("critical_failure"):
            return "END"
        
        # Try to find next step based on result status
        next_step = None
        if result_status in next_steps:
            next_step = next_steps[result_status]
        elif "default" in next_steps:
            next_step = next_steps["default"]
        else:
            next_step = "END"
        
        # Normalize end step variations
        if next_step and next_step.lower() in ['end', 'stop', 'finish', 'complete']:
            next_step = "END"
            
        return next_step
    
def create_multi_agent_workflow(llm: ChatOpenAI, available_tools: List[BaseTool]) -> StateGraph:
    """
    Creates the enhanced multi-agent workflow with ReAct-powered execution.
    """
    planning_agent = PlanningAgent(llm, available_tools)
    execution_agent = ExecutionAgent(llm, available_tools)
    
    # Create the state graph
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("planning", planning_agent.analyze_sop)
    workflow.add_node("execution", execution_agent.execute_step)
    
    # Set entry point
    workflow.set_entry_point("planning")
    
    # Enhanced routing logic
    def route_after_planning(state: AgentState) -> Literal["execution", "__end__"]:
        """Route after planning phase."""
        if state["execution_status"] == "ready_to_execute" and state["execution_queue"]:
            return "execution"
        else:
            return "__end__"
    
    def route_after_execution(state: AgentState) -> Literal["execution", "__end__"]:
        """Route after execution phase."""
        status = state["execution_status"]
        
        if status == "completed":
            print("üéâ Workflow completed successfully!")
            return "__end__"
        elif status == "failed":
            print("üí• Workflow failed - terminating")
            return "__end__"
        elif state["execution_queue"]:
            return "execution"
        else:
            return "__end__"
    
    # Add conditional edges
    workflow.add_conditional_edges(
        "planning",
        route_after_planning,
        {
            "execution": "execution",
            "__end__": END
        }
    )
    
    workflow.add_conditional_edges(
        "execution",
        route_after_execution,
        {
            "execution": "execution",
            "__end__": END
        }
    )
    
    return workflow


# Example usage and testing
if __name__ == "__main__":
    from langchain_core.tools import tool
    
    # Enhanced mock tools for demonstration
    @tool
    def validate_data(data_source: str) -> str:
        """Validates data from the specified source."""
        # Simulate potential failure
        if "invalid" in data_source.lower():
            return f"Data validation FAILED for {data_source} - Status: Invalid data format"
        return f"Data validation COMPLETED for {data_source} - Status: Valid, 1000 records processed"
    
    @tool
    def check_database_connection(database_name: str) -> str:
        """Checks connection to the specified database."""
        # Simulate connection scenarios
        if "test" in database_name.lower():
            return f"Database connection FAILED for {database_name} - Status: Connection timeout"
        return f"Database connection VERIFIED for {database_name} - Status: Connected, latency 45ms"
    
    @tool
    def execute_query(query: str) -> str:
        """Executes a database query."""
        if len(query) < 10:
            return f"Query execution FAILED: {query} - Status: Invalid query format"
        return f"Query executed SUCCESSFULLY: {query} - Rows affected: 150, execution time: 0.3s"
    
    @tool
    def send_notification(message: str, recipient: str) -> str:
        """Sends a notification message."""
        return f"Notification DELIVERED to {recipient}: {message} - Status: Sent via email"
    
    @tool 
    def repair_data(data_source: str, repair_type: str) -> str:
        """Repairs data issues in the specified source."""
        return f"Data repair COMPLETED for {data_source} using {repair_type} - Status: 45 records fixed"
    @tool
    def report_file_creator(errorcode: int, required_fields: list[str]) -> str:
        """
        Creates a report file for a given error code with specified fields.
        Args:
            errorcode (int): The error code for the report.
            required_fields (list[str]): A list of fields to include in the report.
        Returns:
            str: A confirmation message indicating the report file was created.
        """
        # ---- rest of the code -----
        print(f"Tool 'report_file_creator' called with errorcode: {errorcode} and fields: {required_fields}")
        return f"Report file created for error {errorcode}."

    @tool
    def store_in_audit_db(errorcode: int) -> str:
        """
        Stores a record in the audit table based on the provided error code.
        Args:
            errorcode (int): The error code associated with the record to be stored (e.g., 123).
        Returns:
            str: A confirmation message indicating that the data has been successfully saved to the audit table.
        """
        print(f"tool audit called with error code: {errorcode}")
        return "Data saved"
    # Initialize LLM (replace with your preferred LLM)
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    
    # Available tools
    tools = [validate_data, check_database_connection, execute_query, send_notification, repair_data, store_in_audit_db, report_file_creator]

    # Create the enhanced multi-agent workflow
    workflow = create_multi_agent_workflow(llm, tools)
    
    # Compile the graph
    app = workflow.compile()
    
    # Enhanced example SOP content with more complex logic
    sample_sop = """
    SOP for Error Code 935 - Data Validation Failure
    
    OBJECTIVE: Resolve data validation issues and ensure system integrity
    
    WORKFLOW:
    1. VALIDATE DATA SOURCE: Check the integrity of 'customer_data' source
       - IF validation passes: Proceed to step 6 (success notification)
       - IF validation fails: Continue to step 2
    
    2. DIAGNOSE CONNECTION: Verify database connectivity to 'main_db'
       - IF connection is healthy: Proceed to step 3
       - IF connection fails: Send error notification and END
    
    3. REPAIR DATA: Execute data repair using 'cleanup' method on 'customer_data'
       - IF repair succeeds: Proceed to step 4
       - IF repair fails: Send error notification and END
    
    4. RE-VALIDATE: Re-check the 'customer_data' after repair
       - IF validation now passes: Proceed to step 5
       - IF validation still fails: Send failure notification and END
    
    5. EXECUTE MAINTENANCE: Run maintenance query 'UPDATE customers SET status = active WHERE status IS NULL'
       - IF query succeeds: Proceed to step 6
       - IF query fails: Send error notification and END
    
    6. SUCCESS NOTIFICATION: Send success notification to 'admin@company.com'
       - Always END after this step
    
    ERROR HANDLING: For any unexpected errors, send detailed error notification to 'admin@company.com' and END
    """
    
    # Test the enhanced multi-agent system
    initial_state: AgentState = {
        "error_code": 935,
        "sop_content": sample_sop,
        "playbook": None,
        "execution_queue": [],
        "data": None,
        "last_tool_result": None,
        "final_output": {},
        "current_step_id": None,
        "execution_status": "planning",
        "execution_log": [],
        "context_data": {}
    }
    
    # Run the enhanced multi-agent workflow
    try:
        print("üöÄ Starting Enhanced Multi-Agent Workflow with ReAct Pattern...")
        print("=" * 70)
        
        result = app.invoke(initial_state)
        
        print("=" * 70)
        print("üìä FINAL WORKFLOW RESULTS:")
        print(f"Execution Status: {result['execution_status']}")
        print(f"Steps Executed: {len(result.get('execution_log', []))}")
        
        if result.get('execution_log'):
            print("\nüìã DETAILED EXECUTION LOG:")
            for i, log_entry in enumerate(result['execution_log'], 1):
                print(f"\n{i}. STEP: {log_entry['step_name']}")
                print(f"   Objective: {log_entry.get('objective', 'N/A')}")
                print(f"   Result: {log_entry['result'].get('message', 'No message')}")
                print(f"   Status: {log_entry['result'].get('status', 'Unknown')}")
                
                # Show tool results if available
                if log_entry['result'].get('tool_results'):
                    print(f"   Tools Used:")
                    for tool_result in log_entry['result']['tool_results']:
                        print(f"     - {tool_result.get('tool', 'Unknown')}: {tool_result.get('output', 'No output')}")
        
        if result.get('context_data'):
            print(f"\nüîç ACCUMULATED CONTEXT DATA:")
            for key, value in result['context_data'].items():
                print(f"   {key}: {value}")
        
        if result.get('final_output'):
            print(f"\nüéØ FINAL OUTPUT:")
            for key, value in result['final_output'].items():
                print(f"   {key}: {value}")
                
        # Show playbook structure
        if result.get('playbook'):
            print(f"\nüìñ GENERATED PLAYBOOK STRUCTURE:")
            print(f"   Name: {result['playbook'].get('name', 'Unknown')}")
            print(f"   Total Steps: {len(result['playbook'].get('steps', {}))}")
            print(f"   Start Step: {result['playbook'].get('start_step', 'Unknown')}")
            
            print(f"\n   STEP BREAKDOWN:")
            for step_id, step_data in result['playbook'].get('steps', {}).items():
                print(f"     {step_id}: {step_data.get('name', 'Unnamed')} -> {step_data.get('next_steps', {})}")
    
    except Exception as e:
        print(f"‚ùå Error running enhanced multi-agent workflow: {str(e)}")
        import traceback
        traceback.print_exc()