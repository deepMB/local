from typing import TypedDict, List, Optional, Any, Dict, Literal
from langgraph import StateGraph, END
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import BaseTool
from langchain_openai import ChatOpenAI
from langchain.agents import create_react_agent, AgentExecutor
from langchain_core.prompts import PromptTemplate
import json
import re
from pydantic import BaseModel, Field

# Agent State Definition
class AgentState(TypedDict):
    """
    Defines the shared state for the playbook-driven multi-agent system.
    """
    error_code: int
    sop_content: str
    
    playbook: Optional[Dict[str, Any]]
    """The structured JSON workflow graph generated by the Planning Agent."""

    execution_queue: List[str]
    """A queue of node IDs from the playbook that the Execution Agent needs to process."""
    
    data: Optional[Any]
    """Holds operational data, like the pandas DataFrame for error 935."""

    last_tool_result: Optional[Dict[str, Any]]
    """Stores the output from the last executed tool for conditional evaluation."""

    final_output: Dict[str, Any]
    """A dictionary to accumulate final results and statuses from the workflow."""

    current_step_id: Optional[str]
    """Currently executing step ID."""

    execution_status: str
    """Status of execution: 'planning', 'executing', 'completed', 'failed'."""

    execution_log: List[Dict[str, Any]]
    """Log of all executed steps and their results."""

    context_data: Dict[str, Any]
    """Accumulated context data from all executed steps."""


# Playbook Schema for structured output
class PlaybookStep(BaseModel):
    """Represents a single step in the playbook"""
    id: str = Field(description="Unique identifier for the step")
    name: str = Field(description="Human-readable name of the step")
    action: str = Field(description="The action to be performed")
    objective: str = Field(description="What the step is trying to achieve")
    success_criteria: str = Field(description="How to determine if the step was successful")
    failure_criteria: str = Field(description="How to determine if the step failed")
    next_steps: Dict[str, str] = Field(description="Next steps based on conditions (success, failure, etc.)")
    description: str = Field(description="Detailed description of what this step does")
    context_requirements: Optional[List[str]] = Field(description="What context/data is needed for this step")


class PlanningAgent:
    """
    Enhanced Planning Agent that creates ReAct-friendly playbooks.
    """
    
    def __init__(self, llm: ChatOpenAI, available_tools: List[BaseTool]):
        self.llm = llm
        self.available_tools = available_tools
        self.tool_names = [tool.name for tool in available_tools]
        
    def analyze_sop(self, state: AgentState) -> AgentState:
        """
        Analyzes the SOP content and creates a structured playbook optimized for ReAct execution.
        """
        print("üîç Planning Agent: Starting SOP analysis...")
        
        sop_content = state["sop_content"]
        error_code = state["error_code"]
        
        # Update execution status
        state["execution_status"] = "planning"
        state["context_data"] = {}
        
        # Create detailed tool descriptions
        tool_descriptions = []
        for tool in self.available_tools:
            tool_descriptions.append(f"- {tool.name}: {tool.description}")
        
        # Enhanced system prompt for ReAct-optimized planning
        system_prompt = f"""
        You are an expert SOP analyzer that creates ReAct-optimized playbooks for intelligent execution agents.
        
        Your task is to analyze the SOP and create a JSON playbook where each step provides enough context 
        for a ReAct agent to intelligently choose tools and make decisions based on tool outcomes.
        
        AVAILABLE TOOLS:
        {chr(10).join(tool_descriptions)}
        
        CRITICAL DESIGN PRINCIPLES:
        1. Each step should define OBJECTIVES, not specific tools to use
        2. Include clear SUCCESS and FAILURE criteria that a ReAct agent can evaluate
        3. Steps should be tool-agnostic - let the ReAct agent choose the best tool for the objective
        4. Provide rich context about what each step is trying to achieve
        5. Design conditional flows that can handle multiple scenarios
        
        Guidelines for ReAct-optimized playbooks:
        1. Focus on WHAT needs to be achieved, not HOW to achieve it
        2. Provide clear success/failure criteria for each step
        3. Include context requirements (what data/information is needed)
        4. Design steps that allow the ReAct agent to reason about tool selection
        5. Create robust error handling and alternative paths
        6. Each step should be self-contained with clear objectives
        
        Error Code Context: {error_code}
        
        IMPORTANT: Return ONLY the JSON playbook, no other text or explanations.
        """
        
        human_prompt = f"""
        Create a ReAct-optimized JSON playbook for this SOP:
        
        {sop_content}
        
        Required JSON structure:
        {{
            "name": "Playbook name",
            "description": "What this playbook accomplishes",
            "start_step": "first_step_id",
            "steps": {{
                "step_id": {{
                    "id": "step_id",
                    "name": "Step name",
                    "action": "achieve_objective|evaluate_condition|notify|end",
                    "objective": "What this step needs to accomplish",
                    "success_criteria": "How to determine success",
                    "failure_criteria": "How to determine failure", 
                    "next_steps": {{"success": "next_id", "failure": "error_id", "default": "END"}},
                    "description": "Detailed description",
                    "context_requirements": ["required_data1", "required_data2"]
                }}
            }}
        }}
        
        RULES:
        1. Use "END" for terminal steps
        2. Focus on objectives, not specific tools
        3. Include rich success/failure criteria
        4. All referenced step IDs must exist
        5. Make it ReAct agent friendly - provide reasoning context
        """
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt)
        ]
        
        try:
            response = self.llv.invoke(messages)
            playbook_json = self._extract_json_from_response(response.content)
            playbook = self._validate_and_structure_playbook(playbook_json)
            
            state["playbook"] = playbook
            state["execution_queue"] = [playbook["start_step"]] if playbook else []
            state["execution_status"] = "ready_to_execute"
            
            print(f"‚úÖ Planning Agent: Created ReAct-optimized playbook with {len(playbook.get('steps', {}))} steps")
            print(f"üìã Start step: {playbook['start_step']}")
            
        except Exception as e:
            print(f"‚ùå Planning Agent Error: {str(e)}")
            state["playbook"] = None
            state["execution_queue"] = []
            state["execution_status"] = "failed"
            state["final_output"]["error"] = f"Planning failed: {str(e)}"
        
        return state
    
    def _extract_json_from_response(self, response_content: str) -> Dict[str, Any]:
        """Extracts JSON from the LLM response."""
        json_pattern = r'```json\s*(.*?)\s*```'
        json_match = re.search(json_pattern, response_content, re.DOTALL)
        
        if json_match:
            json_str = json_match.group(1)
        else:
            json_str = response_content.strip()
        
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            json_start = response_content.find('{')
            json_end = response_content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_str = response_content[json_start:json_end]
                return json.loads(json_str)
            else:
                raise ValueError(f"Could not extract valid JSON from response: {str(e)}")
    
    def _validate_and_structure_playbook(self, playbook_json: Dict[str, Any]) -> Dict[str, Any]:
        """Validates the playbook structure."""
        if "steps" not in playbook_json:
            raise ValueError("Playbook must contain 'steps' field")
        
        if "start_step" not in playbook_json:
            playbook_json["start_step"] = list(playbook_json["steps"].keys())[0]
        
        if playbook_json["start_step"] not in playbook_json["steps"]:
            raise ValueError(f"Start step '{playbook_json['start_step']}' not found in steps")
        
        # Ensure each step has required fields
        for step_id, step_data in playbook_json["steps"].items():
            if "id" not in step_data:
                step_data["id"] = step_id
            if "next_steps" not in step_data:
                step_data["next_steps"] = {"default": "END"}
            if "action" not in step_data:
                step_data["action"] = "achieve_objective"
            if "success_criteria" not in step_data:
                step_data["success_criteria"] = "Operation completed successfully"
            if "failure_criteria" not in step_data:
                step_data["failure_criteria"] = "Operation failed or error occurred"
            if "context_requirements" not in step_data:
                step_data["context_requirements"] = []
                
            # Normalize end step references
            if "next_steps" in step_data:
                normalized_next_steps = {}
                for condition, next_step in step_data["next_steps"].items():
                    if isinstance(next_step, str) and next_step.lower() in ['end', 'stop', 'finish', 'complete']:
                        normalized_next_steps[condition] = "END"
                    else:
                        normalized_next_steps[condition] = next_step
                step_data["next_steps"] = normalized_next_steps
        
        return playbook_json


class ExecutionAgent:
    """
    Enhanced Execution Agent that uses ReAct pattern for intelligent tool selection and decision making.
    """
    
    def __init__(self, llm: ChatOpenAI, available_tools: List[BaseTool]):
        self.llm = llm
        self.available_tools = available_tools
        self.tool_dict = {tool.name: tool for tool in available_tools}
        
        # Enhanced ReAct prompt for workflow execution
        react_template = """You are an intelligent workflow execution agent. You have access to the following tools:

{tools}

Your role is to execute workflow steps by:
1. Understanding the objective of each step
2. Choosing the most appropriate tool(s) to achieve the objective
3. Evaluating tool results against success/failure criteria
4. Making decisions about next steps based on outcomes

Use the following format:

Question: the workflow step you need to execute
Thought: analyze the objective and determine what needs to be done
Action: choose the most appropriate action from [{tool_names}]
Action Input: provide the necessary input for the action
Observation: analyze the result of the action
... (repeat Thought/Action/Action Input/Observation as needed)
Thought: evaluate if the step objective has been achieved based on success/failure criteria
Final Answer: provide a clear assessment of whether the step succeeded or failed, with reasoning

CRITICAL RULES:
1. If no suitable tool exists for an objective, respond with "NO_SUITABLE_TOOL"
2. If a tool fails or returns unsatisfactory results, try alternative approaches if available
3. Always evaluate results against the provided success/failure criteria
4. If you cannot achieve the objective after trying available tools, respond with "OBJECTIVE_FAILED"

Begin!

Question: {input}
Thought:{agent_scratchpad}"""

        self.react_prompt = PromptTemplate.from_template(react_template)
        
        # Create enhanced ReAct agent
        self.react_agent = create_react_agent(
            llm=self.llm,
            tools=self.available_tools,
            prompt=self.react_prompt
        )
        
        # Create agent executor with better error handling
        self.agent_executor = AgentExecutor(
            agent=self.react_agent,
            tools=self.available_tools,
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=10,  # Increased for more thorough reasoning
            early_stopping_method="generate"
        )
        
    def execute_step(self, state: AgentState) -> AgentState:
        """
        Enhanced step execution using ReAct pattern for intelligent decision making.
        """
        if not state["execution_queue"]:
            state["execution_status"] = "completed"
            return state
            
        current_step_id = state["execution_queue"][0]
        state["current_step_id"] = current_step_id
        
        print(f"üîß Execution Agent: Executing step '{current_step_id}'")
        
        # Handle END step
        if current_step_id.upper() == "END":
            state["execution_status"] = "completed"
            state["execution_queue"] = []
            print("‚úÖ Execution Agent: Workflow completed successfully")
            return state
        
        # Get current step details
        if not state["playbook"] or current_step_id not in state["playbook"]["steps"]:
            if current_step_id.lower() in ['end', 'stop', 'finish', 'complete']:
                state["execution_status"] = "completed"
                state["execution_queue"] = []
                print("‚úÖ Execution Agent: Workflow completed successfully")
                return state
            else:
                state["execution_status"] = "failed"
                state["final_output"]["error"] = f"Step '{current_step_id}' not found in playbook"
                return state
            
        current_step = state["playbook"]["steps"][current_step_id]
        
        try:
            # Execute the step using ReAct pattern
            if current_step["action"] == "achieve_objective":
                result = self._execute_objective_step(current_step, state)
            elif current_step["action"] == "evaluate_condition":
                result = self._evaluate_condition_step(current_step, state)
            elif current_step["action"] == "notify":
                result = self._notify_step(current_step, state)
            else:
                result = {"status": "success", "message": f"Executed step: {current_step['name']}"}
            
            # Check for critical failures
            if result.get("critical_failure"):
                print(f"üõë Critical failure in step '{current_step_id}': {result.get('message')}")
                state["execution_status"] = "failed"
                state["final_output"]["error"] = result.get("message", "Critical failure occurred")
                return state
            
            # Log the execution
            log_entry = {
                "step_id": current_step_id,
                "step_name": current_step.get("name", ""),
                "objective": current_step.get("objective", ""),
                "result": result,
                "timestamp": "now"
            }
            
            if "execution_log" not in state:
                state["execution_log"] = []
            state["execution_log"].append(log_entry)
            
            # Update context data
            if result.get("context_data"):
                state["context_data"].update(result["context_data"])
            
            # Store last tool result
            state["last_tool_result"] = result
            
            # Determine next step using enhanced logic
            next_step_id = self._determine_next_step(current_step, result)
            
            # Update execution queue
            state["execution_queue"] = state["execution_queue"][1:]
            
            if next_step_id and next_step_id != "END":
                state["execution_queue"].insert(0, next_step_id)
            
            print(f"‚úÖ Step '{current_step_id}' completed. Next: '{next_step_id}'")
            
        except Exception as e:
            print(f"‚ùå Execution Agent Error in step '{current_step_id}': {str(e)}")
            state["execution_status"] = "failed"
            state["final_output"]["error"] = f"Execution failed at step '{current_step_id}': {str(e)}"
            
        return state
    
    def _execute_objective_step(self, step: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """
        Execute an objective-based step using ReAct agent for intelligent tool selection.
        """
        objective = step.get("objective", "")
        success_criteria = step.get("success_criteria", "")
        failure_criteria = step.get("failure_criteria", "")
        context_requirements = step.get("context_requirements", [])
        step_description = step.get("description", "")
        
        # Prepare context information
        available_context = {}
        for req in context_requirements:
            if req in state["context_data"]:
                available_context[req] = state["context_data"][req]
        
        # Create comprehensive question for ReAct agent
        question = f"""
        WORKFLOW STEP EXECUTION:
        
        Step: {step.get('name', 'Unnamed Step')}
        Objective: {objective}
        Description: {step_description}
        
        SUCCESS CRITERIA: {success_criteria}
        FAILURE CRITERIA: {failure_criteria}
        
        Available Context:
        {json.dumps(available_context, indent=2) if available_context else "No specific context required"}
        
        Previous Step Result:
        {json.dumps(state.get('last_tool_result', {}), indent=2)}
        
        Your task is to achieve the objective using the available tools. 
        Evaluate your results against the success/failure criteria.
        If you cannot find suitable tools or achieve the objective, clearly state this in your Final Answer.
        """
        
        try:
            print(f"ü§ñ ReAct Agent executing objective: {objective}")
            
            # Use ReAct agent to execute the step
            result = self.agent_executor.invoke({"input": question})
            agent_output = result.get('output', '') if hasattr(result, 'get') else str(result)
            
            print(f"üîç ReAct Agent Output: {agent_output}")
            
            # Enhanced result analysis
            result_analysis = self._analyze_agent_result(agent_output, success_criteria, failure_criteria)
            
            # Check for critical conditions
            if "NO_SUITABLE_TOOL" in agent_output:
                return {
                    "status": "failure",
                    "critical_failure": True,
                    "message": f"No suitable tool available for objective: {objective}",
                    "agent_output": agent_output,
                    "reason": "tool_unavailable"
                }
            
            if "OBJECTIVE_FAILED" in agent_output:
                return {
                    "status": "failure",
                    "critical_failure": True,
                    "message": f"Failed to achieve objective: {objective}",
                    "agent_output": agent_output,
                    "reason": "objective_not_achieved"
                }
            
            # Extract tool results from intermediate steps
            tool_results = []
            context_data = {}
            
            if hasattr(result, 'get') and 'intermediate_steps' in result:
                for step_result in result['intermediate_steps']:
                    if len(step_result) > 1:
                        tool_name = step_result[0].tool if hasattr(step_result[0], 'tool') else 'unknown'
                        tool_output = step_result[1]
                        tool_results.append({
                            "tool": tool_name,
                            "output": tool_output
                        })
                        # Extract potential context data
                        context_data[f"{tool_name}_result"] = tool_output
            
            return {
                "status": result_analysis["status"],
                "objective": objective,
                "success_criteria": success_criteria,
                "failure_criteria": failure_criteria,
                "tool_results": tool_results,
                "agent_output": agent_output,
                "analysis": result_analysis,
                "context_data": context_data,
                "message": result_analysis["message"]
            }
            
        except Exception as e:
            print(f"‚ùå Error in ReAct execution: {str(e)}")
            return {
                "status": "failure",
                "critical_failure": True,
                "objective": objective,
                "error": str(e),
                "message": f"ReAct execution failed: {str(e)}"
            }
    
    def _analyze_agent_result(self, agent_output: str, success_criteria: str, failure_criteria: str) -> Dict[str, Any]:
        """
        Analyze the ReAct agent output to determine success/failure.
        """
        output_lower = agent_output.lower()
        
        # Enhanced success/failure detection
        success_indicators = ['success', 'completed', 'achieved', 'valid', 'connected', 'verified', 'passed']
        failure_indicators = ['failed', 'error', 'invalid', 'disconnected', 'rejected', 'unable', 'cannot']
        
        # Check for explicit success/failure in output
        success_count = sum(1 for indicator in success_indicators if indicator in output_lower)
        failure_count = sum(1 for indicator in failure_indicators if indicator in output_lower)
        
        # Use LLM for more sophisticated analysis if indicators are ambiguous
        if abs(success_count - failure_count) <= 1:  # Ambiguous result
            try:
                analysis_prompt = f"""
                Analyze the following agent output and determine if it indicates success or failure:
                
                Agent Output: {agent_output}
                Success Criteria: {success_criteria}
                Failure Criteria: {failure_criteria}
                
                Respond with exactly: SUCCESS or FAILURE followed by a brief explanation.
                """
                
                analysis_result = self.llm.invoke([HumanMessage(content=analysis_prompt)])
                analysis_text = analysis_result.content.upper()
                
                if "SUCCESS" in analysis_text:
                    return {
                        "status": "success",
                        "message": "Objective achieved based on LLM analysis",
                        "llm_analysis": analysis_result.content
                    }
                else:
                    return {
                        "status": "failure", 
                        "message": "Objective not achieved based on LLM analysis",
                        "llm_analysis": analysis_result.content
                    }
            except:
                pass  # Fall back to simple analysis
        
        # Simple indicator-based analysis
        if success_count > failure_count:
            return {
                "status": "success",
                "message": f"Objective achieved (success indicators: {success_count})",
                "confidence": "indicator_based"
            }
        else:
            return {
                "status": "failure",
                "message": f"Objective not achieved (failure indicators: {failure_count})",
                "confidence": "indicator_based"
            }
    
    def _evaluate_condition_step(self, step: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """Enhanced condition evaluation using ReAct reasoning."""
        objective = step.get("objective", "")
        last_result = state.get("last_tool_result", {})
        context_data = state.get("context_data", {})
        
        evaluation_question = f"""
        CONDITION EVALUATION TASK:
        
        Objective: {objective}
        
        Previous Step Result: {json.dumps(last_result, indent=2)}
        Available Context: {json.dumps(context_data, indent=2)}
        
        Your task is to evaluate whether the condition/objective is met based on the available information.
        Use the available tools if you need to gather additional information.
        
        Provide a clear TRUE or FALSE determination with reasoning.
        """
        
        try:
            result = self.agent_executor.invoke({"input": evaluation_question})
            agent_output = result.get('output', '') if hasattr(result, 'get') else str(result)
            
            # Determine condition result
            condition_met = self._determine_condition_result(agent_output)
            
            return {
                "status": "success" if condition_met else "failure",
                "objective": objective,
                "condition_met": condition_met,
                "agent_output": agent_output,
                "message": f"Condition evaluation: {condition_met}"
            }
            
        except Exception as e:
            return {
                "status": "failure",
                "critical_failure": True,
                "objective": objective,
                "error": str(e),
                "message": f"Condition evaluation failed: {str(e)}"
            }
    
    def _determine_condition_result(self, agent_output: str) -> bool:
        """Determine if condition is met from agent output."""
        output_lower = agent_output.lower()
        
        # Look for explicit true/false statements
        if "true" in output_lower and "false" not in output_lower:
            return True
        if "false" in output_lower and "true" not in output_lower:
            return False
        
        # Look for positive/negative indicators
        positive_indicators = ['yes', 'confirmed', 'met', 'satisfied', 'passed', 'valid']
        negative_indicators = ['no', 'denied', 'not met', 'failed', 'invalid', 'unsatisfied']
        
        positive_count = sum(1 for indicator in positive_indicators if indicator in output_lower)
        negative_count = sum(1 for indicator in negative_indicators if indicator in output_lower)
        
        return positive_count > negative_count
    
    def _notify_step(self, step: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """Enhanced notification step."""
        objective = step.get("objective", "")
        description = step.get("description", "")
        
        print(f"üì¢ Notification: {objective}")
        print(f"   Details: {description}")
        
        return {
            "status": "success",
            "objective": objective,
            "message": f"Notification completed: {objective}"
        }
    
    def _determine_next_step(self, current_step: Dict[str, Any], result: Dict[str, Any]) -> str:
        """Enhanced next step determination."""
        next_steps = current_step.get("next_steps", {})
        result_status = result.get("status", "unknown")
        
        # Check for critical failure - should end workflow
        if result.get("critical_failure"):
            return "END"
        
        # Try to find next step based on result status
        next_step = None
        if result_status in next_steps:
            next_step = next_steps[result_status]
        elif "default" in next_steps:
            next_step = next_steps["default"]
        else:
            next_step = "END"
        
        # Normalize end step variations
        if next_step and next_step.lower() in ['end', 'stop', 'finish', 'complete']:
            next_step = "END"
            
        return next_step


def create_multi_agent_workflow(llm: ChatOpenAI, available_tools: List[BaseTool]) -> StateGraph:
    """
    Creates the enhanced multi-agent workflow with ReAct-powered execution.
    """
    planning_agent = PlanningAgent(llm, available_tools)
    execution_agent = ExecutionAgent(llm, available_tools)
    
    # Create the state graph
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("planning", planning_agent.analyze_sop)
    workflow.add_node("execution", execution_agent.execute_step)
    
    # Set entry point
    workflow.set_entry_point("planning")
    
    # Enhanced routing logic
    def route_after_planning(state: AgentState) -> Literal["execution", "__end__"]:
        """Route after planning phase."""
        if state["execution_status"] == "ready_to_execute" and state["execution_queue"]:
            return "execution"
        else:
            return "__end__"
    
    def route_after_execution(state: AgentState) -> Literal["execution", "__end__"]:
        """Route after execution phase."""
        status = state["execution_status"]
        
        if status == "completed":
            print("üéâ Workflow completed successfully!")
            return "__end__"
        elif status == "failed":
            print("üí• Workflow failed - terminating")
            return "__end__"
        elif state["execution_queue"]:
            return "execution"
        else:
            return "__end__"
    
    # Add conditional edges
    workflow.add_conditional_edges(
        "planning",
        route_after_planning,
        {
            "execution": "execution",
            "__end__": END
        }
    )
    
    workflow.add_conditional_edges(
        "execution",
        route_after_execution,
        {
            "execution": "execution",
            "__end__": END
        }
    )
    
    return workflow


# Example usage and testing
if __name__ == "__main__":
    from langchain_core.tools import tool
    
    # Enhanced mock tools for demonstration
    @tool
    def validate_data(data_source: str) -> str:
        """Validates data from the specified source."""
        # Simulate potential failure
        if "invalid" in data_source.lower():
            return f"Data validation FAILED for {data_source} - Status: Invalid data format"
        return f"Data validation COMPLETED for {data_source} - Status: Valid, 1000 records processed"
    
    @tool
    def check_database_connection(database_name: str) -> str:
        """Checks connection to the specified database."""
        # Simulate connection scenarios
        if "test" in database_name.lower():
            return f"Database connection FAILED for {database_name} - Status: Connection timeout"
        return f"Database connection VERIFIED for {database_name} - Status: Connected, latency 45ms"
    
    @tool
    def execute_query(query: str) -> str:
        """Executes a database query."""
        if len(query) < 10:
            return f"Query execution FAILED: {query} - Status: Invalid query format"
        return f"Query executed SUCCESSFULLY: {query} - Rows affected: 150, execution time: 0.3s"
    
    @tool
    def send_notification(message: str, recipient: str) -> str:
        """Sends a notification message."""
        return f"Notification DELIVERED to {recipient}: {message} - Status: Sent via email"
    
    @tool 
    def repair_data(data_source: str, repair_type: str) -> str:
        """Repairs data issues in the specified source."""
        return f"Data repair COMPLETED for {data_source} using {repair_type} - Status: 45 records fixed"
    
    # Initialize LLM (replace with your preferred LLM)
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    
    # Available tools
    tools = [validate_data, check_database_connection, execute_query, send_notification, repair_data]
    
    # Create the enhanced multi-agent workflow
    workflow = create_multi_agent_workflow(llm, tools)
    
    # Compile the graph
    app = workflow.compile()
    
    # Enhanced example SOP content with more complex logic
    sample_sop = """
    SOP for Error Code 935 - Data Validation Failure
    
    OBJECTIVE: Resolve data validation issues and ensure system integrity
    
    WORKFLOW:
    1. VALIDATE DATA SOURCE: Check the integrity of 'customer_data' source
       - IF validation passes: Proceed to step 6 (success notification)
       - IF validation fails: Continue to step 2
    
    2. DIAGNOSE CONNECTION: Verify database connectivity to 'main_db'
       - IF connection is healthy: Proceed to step 3
       - IF connection fails: Send error notification and END
    
    3. REPAIR DATA: Execute data repair using 'cleanup' method on 'customer_data'
       - IF repair succeeds: Proceed to step 4
       - IF repair fails: Send error notification and END
    
    4. RE-VALIDATE: Re-check the 'customer_data' after repair
       - IF validation now passes: Proceed to step 5
       - IF validation still fails: Send failure notification and END
    
    5. EXECUTE MAINTENANCE: Run maintenance query 'UPDATE customers SET status = active WHERE status IS NULL'
       - IF query succeeds: Proceed to step 6
       - IF query fails: Send error notification and END
    
    6. SUCCESS NOTIFICATION: Send success notification to 'admin@company.com'
       - Always END after this step
    
    ERROR HANDLING: For any unexpected errors, send detailed error notification to 'admin@company.com' and END
    """
    
    # Test the enhanced multi-agent system
    initial_state: AgentState = {
        "error_code": 935,
        "sop_content": sample_sop,
        "playbook": None,
        "execution_queue": [],
        "data": None,
        "last_tool_result": None,
        "final_output": {},
        "current_step_id": None,
        "execution_status": "planning",
        "execution_log": [],
        "context_data": {}
    }
    
    # Run the enhanced multi-agent workflow
    try:
        print("üöÄ Starting Enhanced Multi-Agent Workflow with ReAct Pattern...")
        print("=" * 70)
        
        result = app.invoke(initial_state)
        
        print("=" * 70)
        print("üìä FINAL WORKFLOW RESULTS:")
        print(f"Execution Status: {result['execution_status']}")
        print(f"Steps Executed: {len(result.get('execution_log', []))}")
        
        if result.get('execution_log'):
            print("\nüìã DETAILED EXECUTION LOG:")
            for i, log_entry in enumerate(result['execution_log'], 1):
                print(f"\n{i}. STEP: {log_entry['step_name']}")
                print(f"   Objective: {log_entry.get('objective', 'N/A')}")
                print(f"   Result: {log_entry['result'].get('message', 'No message')}")
                print(f"   Status: {log_entry['result'].get('status', 'Unknown')}")
                
                # Show tool results if available
                if log_entry['result'].get('tool_results'):
                    print(f"   Tools Used:")
                    for tool_result in log_entry['result']['tool_results']:
                        print(f"     - {tool_result.get('tool', 'Unknown')}: {tool_result.get('output', 'No output')}")
        
        if result.get('context_data'):
            print(f"\nüîç ACCUMULATED CONTEXT DATA:")
            for key, value in result['context_data'].items():
                print(f"   {key}: {value}")
        
        if result.get('final_output'):
            print(f"\nüéØ FINAL OUTPUT:")
            for key, value in result['final_output'].items():
                print(f"   {key}: {value}")
                
        # Show playbook structure
        if result.get('playbook'):
            print(f"\nüìñ GENERATED PLAYBOOK STRUCTURE:")
            print(f"   Name: {result['playbook'].get('name', 'Unknown')}")
            print(f"   Total Steps: {len(result['playbook'].get('steps', {}))}")
            print(f"   Start Step: {result['playbook'].get('start_step', 'Unknown')}")
            
            print(f"\n   STEP BREAKDOWN:")
            for step_id, step_data in result['playbook'].get('steps', {}).items():
                print(f"     {step_id}: {step_data.get('name', 'Unnamed')} -> {step_data.get('next_steps', {})}")
    
    except Exception as e:
        print(f"‚ùå Error running enhanced multi-agent workflow: {str(e)}")
        import traceback
        traceback.print_exc()


# Additional utility functions for enhanced functionality
class WorkflowMonitor:
    """Monitor and analyze workflow execution."""
    
    @staticmethod
    def analyze_execution_log(execution_log: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze execution log for insights."""
        if not execution_log:
            return {"status": "no_data", "message": "No execution data available"}
        
        total_steps = len(execution_log)
        successful_steps = sum(1 for log in execution_log if log['result'].get('status') == 'success')
        failed_steps = total_steps - successful_steps
        
        # Identify bottlenecks
        critical_failures = [log for log in execution_log if log['result'].get('critical_failure')]
        
        # Tool usage analysis
        tools_used = {}
        for log in execution_log:
            tool_results = log['result'].get('tool_results', [])
            for tool_result in tool_results:
                tool_name = tool_result.get('tool', 'unknown')
                tools_used[tool_name] = tools_used.get(tool_name, 0) + 1
        
        return {
            "total_steps": total_steps,
            "successful_steps": successful_steps,
            "failed_steps": failed_steps,
            "success_rate": (successful_steps / total_steps) * 100 if total_steps > 0 else 0,
            "critical_failures": len(critical_failures),
            "tools_used": tools_used,
            "bottlenecks": [cf['step_name'] for cf in critical_failures]
        }
    
    @staticmethod
    def generate_execution_report(state: AgentState) -> str:
        """Generate a comprehensive execution report."""
        analysis = WorkflowMonitor.analyze_execution_log(state.get('execution_log', []))
        
        report = f"""
        WORKFLOW EXECUTION REPORT
        =========================
        
        üìä SUMMARY:
        - Total Steps: {analysis['total_steps']}
        - Successful Steps: {analysis['successful_steps']}
        - Failed Steps: {analysis['failed_steps']}
        - Success Rate: {analysis['success_rate']:.1f}%
        - Critical Failures: {analysis['critical_failures']}
        
        üîß TOOL USAGE:
        """
        
        for tool, count in analysis['tools_used'].items():
            report += f"        - {tool}: {count} times\n"
        
        if analysis['bottlenecks']:
            report += f"\n        ‚ö†Ô∏è  BOTTLENECKS DETECTED:\n"
            for bottleneck in analysis['bottlenecks']:
                report += f"        - {bottleneck}\n"
        
        report += f"\n        üìà FINAL STATUS: {state.get('execution_status', 'unknown').upper()}"
        
        return report


# Enhanced error handling and recovery mechanisms
class ErrorRecoveryAgent:
    """Agent for handling errors and implementing recovery strategies."""
    
    def __init__(self, llm: ChatOpenAI, available_tools: List[BaseTool]):
        self.llm = llm
        self.available_tools = available_tools
    
    def analyze_failure(self, failed_step: Dict[str, Any], execution_log: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze failure and suggest recovery strategies."""
        
        failure_analysis_prompt = f"""
        Analyze the following workflow failure and suggest recovery strategies:
        
        Failed Step: {json.dumps(failed_step, indent=2)}
        Execution History: {json.dumps(execution_log[-3:], indent=2)}  # Last 3 steps
        
        Available Tools for Recovery: {[tool.name for tool in self.available_tools]}
        
        Provide:
        1. Root cause analysis
        2. Suggested recovery actions
        3. Alternative approaches
        4. Prevention strategies
        
        Format your response as JSON with keys: root_cause, recovery_actions, alternatives, prevention
        """
        
        try:
            response = self.llm.invoke([HumanMessage(content=failure_analysis_prompt)])
            # Parse the response to extract recovery suggestions
            return {
                "analysis_available": True,
                "llm_response": response.content,
                "timestamp": "now"
            }
        except Exception as e:
            return {
                "analysis_available": False,
                "error": str(e),
                "fallback_suggestion": "Manual intervention required"
            }


# Performance optimization utilities
class WorkflowOptimizer:
    """Optimize workflow execution for better performance."""
    
    @staticmethod
    def optimize_playbook(playbook: Dict[str, Any], execution_history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Optimize playbook based on execution history."""
        if not execution_history:
            return playbook
        
        # Identify frequently failing steps
        failure_patterns = {}
        for log in execution_history:
            if log['result'].get('status') == 'failure':
                step_name = log.get('step_name', 'unknown')
                failure_patterns[step_name] = failure_patterns.get(step_name, 0) + 1
        
        # Add retry logic for frequently failing steps
        optimized_playbook = playbook.copy()
        
        for step_id, step_data in optimized_playbook.get('steps', {}).items():
            step_name = step_data.get('name', '')
            if step_name in failure_patterns and failure_patterns[step_name] > 1:
                # Add retry mechanism
                step_data['retry_count'] = 3
                step_data['retry_delay'] = 1  # seconds
                print(f"üîß Added retry logic to step: {step_name}")
        
        return optimized_playbook


# Example of how to use the enhanced system with all features
def run_enhanced_workflow_example():
    """Complete example with all enhanced features."""
    
    # Initialize components
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    tools = [validate_data, check_database_connection, execute_query, send_notification, repair_data]
    
    workflow = create_multi_agent_workflow(llm, tools)
    app = workflow.compile()
    
    # Initialize monitoring and recovery
    monitor = WorkflowMonitor()
    recovery_agent = ErrorRecoveryAgent(llm, tools)
    optimizer = WorkflowOptimizer()
    
    # Complex SOP for testing
    complex_sop = """
    SOP for Critical System Recovery - Error Code 935
    
    OBJECTIVE: Implement comprehensive data validation and system recovery
    
    WORKFLOW:
    1. INITIAL ASSESSMENT: Validate 'customer_data' source integrity
    2. CONNECTION VERIFICATION: Ensure 'main_db' database accessibility  
    3. DATA REPAIR: If needed, repair data using appropriate methods
    4. VALIDATION CYCLE: Re-validate after repairs
    5. SYSTEM MAINTENANCE: Execute necessary maintenance queries
    6. FINAL VERIFICATION: Confirm all systems operational
    7. NOTIFICATION: Alert administrators of completion status
    
    CONDITIONAL LOGIC:
    - IF initial validation passes: Skip to final verification
    - IF connection fails: Implement connection recovery procedures
    - IF repair fails: Escalate to manual intervention
    - IF any critical step fails: Immediate notification and halt
    
    SUCCESS CRITERIA: All validations pass, system operational
    FAILURE CRITERIA: Any critical component fails or is inaccessible
    """
    
    initial_state: AgentState = {
        "error_code": 935,
        "sop_content": complex_sop,
        "playbook": None,
        "execution_queue": [],
        "data": None,
        "last_tool_result": None,
        "final_output": {},
        "current_step_id": None,
        "execution_status": "planning",
        "execution_log": [],
        "context_data": {}
    }
    
    try:
        print("üöÄ RUNNING COMPREHENSIVE ENHANCED WORKFLOW...")
        print("=" * 80)
        
        # Execute workflow
        result = app.invoke(initial_state)
        
        # Generate comprehensive report
        report = monitor.generate_execution_report(result)
        print(report)
        
        # Analyze execution
        analysis = monitor.analyze_execution_log(result.get('execution_log', []))
        print(f"\nüìà EXECUTION ANALYSIS:")
        print(f"   Success Rate: {analysis['success_rate']:.1f}%")
        print(f"   Tools Utilized: {list(analysis['tools_used'].keys())}")
        
        # If there were failures, run recovery analysis
        if analysis['critical_failures'] > 0:
            print(f"\nüîç FAILURE ANALYSIS:")
            failed_steps = [log for log in result.get('execution_log', []) 
                          if log['result'].get('critical_failure')]
            
            for failed_step in failed_steps:
                recovery_analysis = recovery_agent.analyze_failure(
                    failed_step, result.get('execution_log', [])
                )
                print(f"   Recovery suggestion available: {recovery_analysis['analysis_available']}")
        
        # Optimize playbook for future runs
        if result.get('playbook') and result.get('execution_log'):
            optimized_playbook = optimizer.optimize_playbook(
                result['playbook'], result['execution_log']
            )
            print(f"\nüîß PLAYBOOK OPTIMIZED: {len(optimized_playbook.get('steps', {}))} steps enhanced")
        
        return result
        
    except Exception as e:
        print(f"‚ùå COMPREHENSIVE WORKFLOW ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    # Run the comprehensive example
    run_enhanced_workflow_example()