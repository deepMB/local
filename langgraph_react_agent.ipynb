{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c3b58db",
   "metadata": {},
   "source": [
    "# LangGraph Agent: LLM-Guided ReAct Workflow Executor\n",
    "\n",
    "This notebook demonstrates a multi-agent LangGraph system:\n",
    "\n",
    "- ðŸ§  `planner`: Generates a JSON playbook based on SOP\n",
    "- ðŸ¤– `decider`: Uses LLM to determine next tool(s) to execute\n",
    "- ðŸ› ï¸ `executor`: Calls tools from a registry\n",
    "\n",
    "The system loops through the playbook nodes based on conditions and tool results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49828ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LangChain and LangGraph if needed\n",
    "# %pip install langchain langgraph openai\n",
    "import json\n",
    "from typing import TypedDict, List, Optional, Any, Dict, Callable\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Mock LLM (replace with actual API)\n",
    "llm = ChatOpenAI(model='gpt-4', temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f00d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AgentState Definition\n",
    "class AgentState(TypedDict):\n",
    "    error_code: int\n",
    "    sop_content: str\n",
    "    input: str\n",
    "    plan: dict\n",
    "    executed_steps: List[str]\n",
    "    current_step_index: Any\n",
    "    tool_results: Optional[dict]\n",
    "    data: Optional[Any]\n",
    "    agent_outcome: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b26f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def timestamp_comparison_935():\n",
    "    # Dummy logic\n",
    "    return {\"flag_1_count\": 10, \"flag_0_count\": 5}\n",
    "\n",
    "@tool\n",
    "def create_rcl_file_935():\n",
    "    return {\"file\": \"RCL.xlsx\"}\n",
    "\n",
    "@tool\n",
    "def create_research_file_935():\n",
    "    return {\"file\": \"research.xlsx\"}\n",
    "\n",
    "@tool\n",
    "def send_email(file_type: str):\n",
    "    return {\"status\": f\"{file_type} file emailed\"}\n",
    "@tool\n",
    "def create_servicenow_ticket(file_type: str):\n",
    "    return {\"status\": f\"{file_type} ticket created\"}\n",
    "@tool\n",
    "def create_servicenow_ticket_935(file_type: str):\n",
    "    return create_servicenow_ticket(file_type)\n",
    "tool_registry = {\n",
    "    \"timestamp_comparison_935\": timestamp_comparison_935,\n",
    "    \"create_rcl_file_935\": create_rcl_file_935,\n",
    "    \"create_research_file_935\": create_research_file_935,\n",
    "    \"send_email\": send_email,\n",
    "    \"create_servicenow_ticket\": create_servicenow_ticket\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521483f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_playbook_planner_node(llm, tool_descriptions) -> Callable[[AgentState], AgentState]:\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a planning agent. Given the SOP content and available tools, generate a workflow in JSON format.\n",
    "\n",
    "SOP:\n",
    "{sop}\n",
    "\n",
    "Available Tools:\n",
    "{tools}\n",
    "\n",
    "Return a JSON object in this format:\n",
    "{{\n",
    "  \"startNode\": \"node-1\",\n",
    "  \"nodes\": {{\n",
    "    \"node-1\": {{\n",
    "      \"toolName\": \"...\",\n",
    "      \"description\": \"...\",\n",
    "      \"on_success\": \"...\"\n",
    "    }},\n",
    "    ...\n",
    "  }}\n",
    "}}\n",
    "\"\"\")\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    def plan(state: AgentState) -> AgentState:\n",
    "        input_data = {\n",
    "            \"sop\": state[\"sop_content\"],\n",
    "            \"tools\": tool_descriptions\n",
    "        }\n",
    "        response = chain.invoke(input_data)\n",
    "\n",
    "        # ðŸ”§ Extract text from AIMessage\n",
    "        response_text = response.content if hasattr(response, \"content\") else str(response)\n",
    "\n",
    "        # âœ… Parse JSON\n",
    "        playbook = json.loads(response_text)\n",
    "\n",
    "        state[\"plan\"] = playbook\n",
    "        state[\"current_step_index\"] = playbook[\"startNode\"]\n",
    "        return state\n",
    "\n",
    "    return plan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a3265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "def get_llm_path_decider_node(llm) -> Runnable:\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a flow controller for an agentic system.\n",
    "\n",
    "Here is the JSON playbook plan (simplified):\n",
    "{plan}\n",
    "\n",
    "The last tool node executed was `{current_node_id}`.\n",
    "Here is the result:\n",
    "{tool_result}\n",
    "\n",
    "Using this, decide the **next node or nodes to run**.\n",
    "\n",
    "Respond in JSON as:\n",
    "{{ \"next_nodes\": [\"node-2\"] }}\n",
    "OR\n",
    "{{ \"next_nodes\": [\"node-2\", \"node-3\"] }}  (if both branches should run)\n",
    "\n",
    "If execution should stop, respond:\n",
    "{{ \"next_nodes\": [] }}\n",
    "\"\"\")\n",
    "\n",
    "    chain = prompt | llm | (lambda output: json.loads(output))\n",
    "    \n",
    "    def decide_path(state: AgentState) -> AgentState:\n",
    "        current_node = state[\"current_step_index\"]\n",
    "        plan = state[\"plan\"]\n",
    "        tool_result = state.get(\"tool_results\", {})\n",
    "        response = chain.invoke({\n",
    "            \"plan\": json.dumps(plan[\"nodes\"], indent=2),\n",
    "            \"current_node_id\": current_node,\n",
    "            \"tool_result\": json.dumps(tool_result)\n",
    "        })\n",
    "        state[\"agent_outcome\"] = \"Next: \" + \", \".join(response[\"next_nodes\"])\n",
    "        state[\"current_step_index\"] = response[\"next_nodes\"]\n",
    "        return state\n",
    "\n",
    "    return decide_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_react_executor_node(tool_registry: Dict[str, Callable]) -> Callable[[AgentState], AgentState]:\n",
    "    def executor(state: AgentState) -> AgentState:\n",
    "        plan = state[\"plan\"]\n",
    "        nodes = plan[\"nodes\"]\n",
    "        current_nodes = state.get(\"current_step_index\", [plan[\"startNode\"]])\n",
    "        if isinstance(current_nodes, str):\n",
    "            current_nodes = [current_nodes]\n",
    "        executed = state.get(\"executed_steps\", [])\n",
    "        \n",
    "        for node_id in current_nodes:\n",
    "            if node_id in executed or node_id == \"end\":\n",
    "                continue\n",
    "\n",
    "            node = nodes.get(node_id)\n",
    "            if not node:\n",
    "                raise RuntimeError(f\"Node '{node_id}' not found.\")\n",
    "\n",
    "            tool_name = node.get(\"toolName\")\n",
    "            if not tool_name:\n",
    "                raise RuntimeError(f\"No toolName in node '{node_id}'.\")\n",
    "\n",
    "            tool = tool_registry.get(tool_name)\n",
    "            if not tool:\n",
    "                raise RuntimeError(f\"Tool '{tool_name}' not found in registry.\")\n",
    "\n",
    "            params = node.get(\"parameters\", {})\n",
    "            result = tool(**params)\n",
    "\n",
    "            state[\"tool_results\"] = result\n",
    "            executed.append(node_id)\n",
    "\n",
    "        state[\"executed_steps\"] = executed\n",
    "        return state\n",
    "\n",
    "    return executor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d11d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Add all nodes\n",
    "builder.add_node(\"planner\", get_custom_json_planner_node(llm, available_tools))\n",
    "builder.add_node(\"decider\", get_llm_path_decider_node(llm))  # Reads the plan and tool results\n",
    "builder.add_node(\"executor\", get_llm_react_executor_node(tool_registry))  # Executes selected node(s)\n",
    "\n",
    "# Correct flow:\n",
    "builder.set_entry_point(\"planner\")\n",
    "builder.add_edge(\"planner\", \"decider\")     # planner -> decider (read plan, get next nodes)\n",
    "builder.add_edge(\"decider\", \"executor\")    # decider -> executor (runs tools)\n",
    "builder.add_edge(\"executor\", \"decider\")    # loop executor -> decider\n",
    "builder.set_finish_point(\"END\")\n",
    "\n",
    "builder.add_edge(\"executor\", \"decider\")\n",
    "builder.set_finish_point(\"decider\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cd088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "def get_llm_path_decider_node(llm) -> Callable[[AgentState], Any]:\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a controller deciding the next step in a playbook.\n",
    "\n",
    "Playbook:\n",
    "{plan}\n",
    "Current node: {current_node_id}\n",
    "Tool result:\n",
    "{tool_result}\n",
    "\n",
    "Return {\"next_nodes\": [\"node-2\"]} for next steps, or [] to finish the plan.\n",
    "\"\"\")\n",
    "\n",
    "    chain = prompt | llm | (lambda x: json.loads(x))\n",
    "\n",
    "    def decide(state: AgentState) -> Any:\n",
    "        current = state[\"current_step_index\"]\n",
    "        if isinstance(current, list): current = current[0]\n",
    "\n",
    "        response = chain.invoke({\n",
    "            \"plan\": json.dumps(state[\"plan\"], indent=2),\n",
    "            \"current_node_id\": current,\n",
    "            \"tool_result\": json.dumps(state.get(\"tool_results\", {}))\n",
    "        })\n",
    "\n",
    "        next_nodes = response.get(\"next_nodes\", [])\n",
    "\n",
    "        if not next_nodes:\n",
    "            return END  # ðŸ‘ˆ This ends the graph\n",
    "\n",
    "        state[\"current_step_index\"] = next_nodes\n",
    "        return state\n",
    "\n",
    "    return decide\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
