from typing import TypedDict, List, Optional, Any, Dict, Literal
from langgraph import StateGraph, END
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import BaseTool
from langchain_openai import ChatOpenAI
from langchain.agents import create_react_agent, AgentExecutor
from langchain_core.prompts import PromptTemplate
import json
import re
from pydantic import BaseModel, Field

# Agent State Definition
class AgentState(TypedDict):
    """
    Defines the shared state for the playbook-driven multi-agent system.
    """
    error_code: int
    sop_content: str
    
    playbook: Optional[Dict[str, Any]]
    """The structured JSON workflow graph generated by the Planning Agent."""

    execution_queue: List[str]
    """A queue of node IDs from the playbook that the Execution Agent needs to process."""
    
    data: Optional[Any]
    """Holds operational data, like the pandas DataFrame for error 935."""

    last_tool_result: Optional[Dict[str, Any]]
    """Stores the output from the last executed tool for conditional evaluation."""

    final_output: Dict[str, Any]
    """A dictionary to accumulate final results and statuses from the workflow."""

    current_step_id: Optional[str]
    """Currently executing step ID."""

    execution_status: str
    """Status of execution: 'planning', 'executing', 'completed', 'failed'."""

    execution_log: List[Dict[str, Any]]
    """Log of all executed steps and their results."""


class PlanningAgent:
    """
    Planning Agent that reads SOPs and creates structured JSON playbooks.
    """
    
    def __init__(self, llm: ChatOpenAI, available_tools: List[BaseTool]):
        self.llm = llm
        self.available_tools = available_tools
        self.tool_names = [tool.name for tool in available_tools]
        
    def analyze_sop(self, state: AgentState) -> AgentState:
        """
        Analyzes the SOP content and creates a structured playbook.
        """
        print("üîç Planning Agent: Starting SOP analysis...")
        
        sop_content = state["sop_content"]
        error_code = state["error_code"]
        
        # Update execution status
        state["execution_status"] = "planning"
        
        # Create detailed tool descriptions with explicit parameter information
        tool_descriptions = []
        for tool in self.available_tools:
            # Get tool schema for better understanding
            tool_schema = tool.args_schema
            if tool_schema:
                # Extract parameter info from schema
                params_info = []
                if hasattr(tool_schema, 'model_fields'):
                    for field_name, field_info in tool_schema.model_fields.items():
                        field_type = getattr(field_info.annotation, '__name__', str(field_info.annotation))
                        params_info.append(f"{field_name}: {field_type}")
                
                param_str = f" (Parameters: {', '.join(params_info)})" if params_info else ""
            else:
                param_str = ""
            
            tool_descriptions.append(f"- {tool.name}: {tool.description}{param_str}")
        
        # Create the system prompt for SOP analysis
        system_prompt = f"""
        You are an expert SOP (Standard Operating Procedure) analyzer and workflow designer.
        
        Your task is to analyze the given SOP content and create a structured JSON playbook that can be executed by an automation agent.
        
        AVAILABLE TOOLS (YOU MUST ONLY USE THESE TOOLS):
        {chr(10).join(tool_descriptions)}
        
        CRITICAL RULES FOR TOOL USAGE:
        1. You can ONLY use tools from the available tools list above
        2. Do NOT create, invent, or reference any tools not in the list
        3. Use EXACT tool names as shown in the list (case-sensitive)
        4. Ensure tool arguments match the expected parameter types and names
        5. If a step requires a tool that's not available, use "action": "notify" instead
        6. Map each SOP step to the most appropriate available tool
        
        Guidelines for creating the playbook:
        1. Break down the SOP into discrete, actionable steps
        2. Identify decision points and conditional logic (IF/ELSE scenarios)
        3. Map each step to appropriate tools where applicable (ONLY from available tools)
        4. Create a flow that handles both success and failure paths
        5. Ensure each step has clear next steps based on outcomes
        6. Use meaningful IDs for steps (e.g., "step_001_validate_data", "step_002_check_connection")
        
        For conditional steps:
        - Use "condition" field to specify what to evaluate
        - Use "next_steps" to define paths: {{"success": "next_step_id", "failure": "error_step_id", "default": "default_step_id"}}
        
        The playbook should be comprehensive enough that an execution agent can follow it step-by-step without ambiguity.
        
        Error Code Context: {error_code}
        
        IMPORTANT: Return ONLY the JSON playbook, no other text or explanations.
        """
        
        human_prompt = f"""
        Create a detailed JSON playbook for this SOP:
        
        {sop_content}
        
        Required JSON structure:
        {{
            "name": "Playbook name",
            "description": "What this playbook does",
            "start_step": "first_step_id",
            "steps": {{
                "step_id": {{
                    "id": "step_id",
                    "name": "Step name",
                    "action": "execute_tool|evaluate_condition|notify|end",
                    "tool_name": "exact_tool_name_from_available_list",
                    "tool_args": {{"parameter_name": "value"}},
                    "condition": "condition_to_evaluate_if_applicable",
                    "next_steps": {{"success": "next_id", "failure": "error_id", "default": "END"}},
                    "description": "What this step does"
                }}
            }}
        }}
        
        IMPORTANT RULES:
        1. Use "END" (uppercase) for terminal steps in next_steps
        2. Never use "end", "stop", "finish" - always use "END"
        3. Ensure all referenced step IDs actually exist in the steps dictionary
        4. Every step must have a valid next_steps path
        5. Use EXACT tool names from the available tools list
        6. Ensure tool arguments match expected parameter names and types
        """
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt)
        ]
        
        try:
            # Get structured output from LLM
            response = self.llm.invoke(messages)
            
            # Parse the response to extract JSON
            playbook_json = self._extract_json_from_response(response.content)
            
            # Validate and structure the playbook
            playbook = self._validate_and_structure_playbook(playbook_json)
            
            # Update state with the created playbook
            state["playbook"] = playbook
            state["execution_queue"] = [playbook["start_step"]] if playbook else []
            state["execution_status"] = "ready_to_execute"
            
            print(f"‚úÖ Planning Agent: Successfully created playbook with {len(playbook.get('steps', {}))} steps")
            print(f"üìã Start step: {playbook['start_step']}")
            
        except Exception as e:
            print(f"‚ùå Planning Agent Error: {str(e)}")
            state["playbook"] = None
            state["execution_queue"] = []
            state["execution_status"] = "failed"
            state["final_output"]["error"] = f"Planning failed: {str(e)}"
        
        return state
    
    def _extract_json_from_response(self, response_content: str) -> Dict[str, Any]:
        """Extracts JSON from the LLM response."""
        # Try to find JSON block in the response
        json_pattern = r'```json\s*(.*?)\s*```'
        json_match = re.search(json_pattern, response_content, re.DOTALL)
        
        if json_match:
            json_str = json_match.group(1)
        else:
            # Try to find JSON without code blocks
            json_str = response_content.strip()
        
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            # Fallback: try to extract JSON from anywhere in the text
            json_start = response_content.find('{')
            json_end = response_content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_str = response_content[json_start:json_end]
                return json.loads(json_str)
            else:
                raise ValueError(f"Could not extract valid JSON from response: {str(e)}")
    
    def _validate_and_structure_playbook(self, playbook_json: Dict[str, Any]) -> Dict[str, Any]:
        """Validates the playbook structure and ensures only valid tools are used."""
        if "steps" not in playbook_json:
            raise ValueError("Playbook must contain 'steps' field")
        
        if "start_step" not in playbook_json:
            playbook_json["start_step"] = list(playbook_json["steps"].keys())[0]
        
        # Validate that start_step exists in steps
        if playbook_json["start_step"] not in playbook_json["steps"]:
            raise ValueError(f"Start step '{playbook_json['start_step']}' not found in steps")
        
        # Ensure each step has required fields and validate tool usage
        for step_id, step_data in playbook_json["steps"].items():
            if "id" not in step_data:
                step_data["id"] = step_id
            if "next_steps" not in step_data:
                step_data["next_steps"] = {"default": "END"}
            if "action" not in step_data:
                step_data["action"] = "execute_tool"
                
            # Validate tool usage - CRITICAL FIX
            if step_data.get("tool_name") and step_data["tool_name"] not in self.tool_names:
                print(f"‚ö†Ô∏è  Warning: Invalid tool '{step_data['tool_name']}' in step '{step_id}'. Converting to notification step.")
                step_data["action"] = "notify"
                step_data["description"] = f"Manual step required: {step_data.get('description', 'Execute manually')}"
                if "tool_name" in step_data:
                    del step_data["tool_name"]
                if "tool_args" in step_data:
                    del step_data["tool_args"]
                
            # Normalize any end step references to uppercase END
            if "next_steps" in step_data:
                normalized_next_steps = {}
                for condition, next_step in step_data["next_steps"].items():
                    if isinstance(next_step, str) and next_step.lower() in ['end', 'stop', 'finish', 'complete']:
                        normalized_next_steps[condition] = "END"
                    else:
                        normalized_next_steps[condition] = next_step
                step_data["next_steps"] = normalized_next_steps
        
        return playbook_json


class ExecutionAgent:
    """
    Enhanced Execution Agent with improved tool calling and error handling.
    """
    
    def __init__(self, llm: ChatOpenAI, available_tools: List[BaseTool]):
        self.llm = llm
        self.available_tools = available_tools
        self.tool_dict = {tool.name: tool for tool in available_tools}
        
    def execute_step(self, state: AgentState) -> AgentState:
        """
        Executes the current step with improved tool handling.
        """
        if not state["execution_queue"]:
            state["execution_status"] = "completed"
            return state
            
        current_step_id = state["execution_queue"][0]
        state["current_step_id"] = current_step_id
        
        print(f"üîß Execution Agent: Processing step '{current_step_id}'")
        
        # Handle END step (case-insensitive)
        if current_step_id.upper() == "END":
            state["execution_status"] = "completed"
            state["execution_queue"] = []
            print("‚úÖ Execution Agent: Workflow completed successfully")
            return state
        
        # Get current step details
        if not state["playbook"] or current_step_id not in state["playbook"]["steps"]:
            # Check if it's a variation of END
            if current_step_id.lower() in ['end', 'stop', 'finish', 'complete']:
                state["execution_status"] = "completed"
                state["execution_queue"] = []
                print("‚úÖ Execution Agent: Workflow completed successfully")
                return state
            else:
                state["execution_status"] = "failed"
                state["final_output"]["error"] = f"Step '{current_step_id}' not found in playbook"
                return state
            
        current_step = state["playbook"]["steps"][current_step_id]
        
        try:
            # Execute the step based on action type
            if current_step["action"] == "execute_tool":
                result = self._execute_tool_directly(current_step, state)
            elif current_step["action"] == "evaluate_condition":
                result = self._evaluate_condition(current_step, state)
            elif current_step["action"] == "notify":
                result = self._notify_step(current_step, state)
            else:
                result = {"status": "success", "message": f"Executed step: {current_step['name']}"}
            
            # Log the execution
            log_entry = {
                "step_id": current_step_id,
                "step_name": current_step.get("name", ""),
                "result": result,
                "timestamp": "now"
            }
            
            if "execution_log" not in state:
                state["execution_log"] = []
            state["execution_log"].append(log_entry)
            
            # Store last tool result
            state["last_tool_result"] = result
            
            # Determine next step
            next_step_id = self._determine_next_step(current_step, result)
            
            # Update execution queue
            state["execution_queue"] = state["execution_queue"][1:]  # Remove current step
            
            if next_step_id and next_step_id != "END":
                state["execution_queue"].insert(0, next_step_id)  # Add next step to front
            
            print(f"‚úÖ Step '{current_step_id}' completed. Next: '{next_step_id}'")
            
        except Exception as e:
            print(f"‚ùå Execution Agent Error in step '{current_step_id}': {str(e)}")
            state["execution_status"] = "failed"
            state["final_output"]["error"] = f"Execution failed at step '{current_step_id}': {str(e)}"
            
        return state
    
    def _execute_tool_directly(self, step: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """
        Direct tool execution with proper error handling and validation.
        """
        tool_name = step.get("tool_name")
        tool_args = step.get("tool_args", {})
        step_name = step.get("name", "Unnamed Step")
        
        print(f"üõ†Ô∏è  Executing tool: {tool_name} with args: {tool_args}")
        
        # Validate tool exists
        if not tool_name:
            return {
                "status": "failure",
                "error": "No tool specified for this step",
                "message": "Tool execution failed: No tool specified"
            }
        
        if tool_name not in self.tool_dict:
            return {
                "status": "failure",
                "error": f"Tool '{tool_name}' not found in available tools",
                "message": f"Tool execution failed: '{tool_name}' not available",
                "available_tools": list(self.tool_dict.keys())
            }
        
        tool = self.tool_dict[tool_name]
        
        try:
            # Validate and clean tool arguments
            cleaned_args = self._validate_and_clean_tool_args(tool, tool_args)
            
            print(f"üîß Calling {tool_name} with validated args: {cleaned_args}")
            
            # Execute the tool directly
            if cleaned_args:
                tool_result = tool.invoke(cleaned_args)
            else:
                tool_result = tool.invoke({})
            
            print(f"‚úÖ Tool {tool_name} executed successfully: {tool_result}")
            
            # Determine success based on result
            is_successful = self._determine_tool_success(tool_result)
            
            return {
                "status": "success" if is_successful else "failure",
                "tool_used": tool_name,
                "tool_args": cleaned_args,
                "tool_result": tool_result,
                "message": f"Tool '{tool_name}' executed {'successfully' if is_successful else 'with issues'}"
            }
            
        except Exception as e:
            error_msg = str(e)
            print(f"‚ùå Tool execution error: {error_msg}")
            
            return {
                "status": "failure",
                "tool_used": tool_name,
                "tool_args": tool_args,
                "error": error_msg,
                "message": f"Tool '{tool_name}' execution failed: {error_msg}"
            }
    
    def _validate_and_clean_tool_args(self, tool: BaseTool, tool_args: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate and clean tool arguments based on tool schema.
        """
        if not tool_args:
            return {}
        
        # If tool has no args schema, return args as-is
        if not hasattr(tool, 'args_schema') or not tool.args_schema:
            return tool_args
        
        cleaned_args = {}
        
        try:
            # Get expected fields from the tool's schema
            if hasattr(tool.args_schema, 'model_fields'):
                expected_fields = tool.args_schema.model_fields
                
                for field_name, field_info in expected_fields.items():
                    if field_name in tool_args:
                        # Basic type validation/conversion
                        value = tool_args[field_name]
                        
                        # Handle string conversion for basic types
                        if hasattr(field_info, 'annotation'):
                            expected_type = field_info.annotation
                            
                            # Handle Optional types
                            if hasattr(expected_type, '__origin__') and expected_type.__origin__ is type(None).__class__.__base__:
                                # This is Optional[SomeType]
                                if hasattr(expected_type, '__args__'):
                                    expected_type = expected_type.__args__[0]
                            
                            # Convert if necessary
                            if expected_type == str and not isinstance(value, str):
                                value = str(value)
                            elif expected_type == int and not isinstance(value, int):
                                try:
                                    value = int(value)
                                except (ValueError, TypeError):
                                    pass  # Keep original value
                            elif expected_type == float and not isinstance(value, float):
                                try:
                                    value = float(value)
                                except (ValueError, TypeError):
                                    pass  # Keep original value
                        
                        cleaned_args[field_name] = value
                
                # Check for required fields that are missing
                for field_name, field_info in expected_fields.items():
                    if field_name not in cleaned_args:
                        # Check if field is required (not Optional)
                        is_optional = (hasattr(field_info, 'default') and field_info.default is not None) or \
                                    (hasattr(field_info, 'annotation') and 
                                     hasattr(field_info.annotation, '__origin__') and 
                                     field_info.annotation.__origin__ is type(None).__class__.__base__)
                        
                        if not is_optional:
                            print(f"‚ö†Ô∏è  Missing required argument '{field_name}' for tool '{tool.name}'")
            else:
                # Fallback to original args if schema inspection fails
                cleaned_args = tool_args
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Error validating tool args: {str(e)}. Using original args.")
            cleaned_args = tool_args
        
        return cleaned_args
    
    def _determine_tool_success(self, tool_result: Any) -> bool:
        """
        Determine if tool execution was successful based on the result.
        """
        if tool_result is None:
            return False
        
        result_str = str(tool_result).lower()
        
        # Check for explicit failure indicators
        failure_indicators = ['error', 'failed', 'exception', 'timeout', 'invalid', 'not found', 'denied']
        if any(indicator in result_str for indicator in failure_indicators):
            return False
        
        # Check for explicit success indicators
        success_indicators = ['success', 'completed', 'valid', 'connected', 'ok', 'delivered', 'executed']
        if any(indicator in result_str for indicator in success_indicators):
            return True
        
        # If no clear indicators, assume success if result is not empty/None
        return bool(tool_result and str(tool_result).strip())
    
    def _evaluate_condition(self, step: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """
        Evaluate condition based on previous results.
        """
        condition = step.get("condition", "")
        last_result = state.get("last_tool_result", {})
        
        print(f"üîç Evaluating condition: {condition}")
        print(f"üìä Last result: {last_result}")
        
        # Simple condition evaluation based on last result status
        condition_met = False
        
        if "success" in condition.lower():
            condition_met = last_result.get("status") == "success"
        elif "failure" in condition.lower() or "failed" in condition.lower():
            condition_met = last_result.get("status") == "failure"
        else:
            # Default: consider condition met if last step was successful
            condition_met = last_result.get("status") == "success"
        
        print(f"‚úÖ Condition '{condition}' evaluated to: {condition_met}")
        
        return {
            "status": "success" if condition_met else "failure",
            "condition": condition,
            "condition_met": condition_met,
            "message": f"Condition '{condition}' evaluated to {condition_met}"
        }
    
    def _determine_next_step(self, current_step: Dict[str, Any], result: Dict[str, Any]) -> str:
        """
        Determine the next step based on current result.
        """
        next_steps = current_step.get("next_steps", {})
        result_status = result.get("status", "unknown")
        
        print(f"üîÄ Determining next step. Result status: {result_status}")
        print(f"üìã Available next steps: {next_steps}")
        
        # Try to match result status with next steps
        if result_status in next_steps:
            next_step = next_steps[result_status]
        elif "default" in next_steps:
            next_step = next_steps["default"]
        else:
            next_step = "END"
        
        print(f"‚û°Ô∏è  Next step determined: {next_step}")
        return next_step
    
    def _notify_step(self, step: Dict[str, Any], state: AgentState) -> Dict[str, Any]:
        """Handle notification steps."""
        message = step.get("description", "Notification step executed")
        
        print(f"üì¢ Notification: {message}")
        
        return {
            "status": "success",
            "message": f"Notification sent: {message}"
        }


def create_multi_agent_workflow(llm: ChatOpenAI, available_tools: List[BaseTool]) -> StateGraph:
    """
    Creates the multi-agent workflow with both Planning and Execution agents.
    """
    planning_agent = PlanningAgent(llm, available_tools)
    execution_agent = ExecutionAgent(llm, available_tools)
    
    # Create the state graph
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("planning", planning_agent.analyze_sop)
    workflow.add_node("execution", execution_agent.execute_step)
    
    # Set entry point
    workflow.set_entry_point("planning")
    
    # Define routing logic
    def route_after_planning(state: AgentState) -> Literal["execution", "__end__"]:
        """Route after planning phase."""
        if state["execution_status"] == "ready_to_execute" and state["execution_queue"]:
            return "execution"
        else:
            return "__end__"
    
    def route_after_execution(state: AgentState) -> Literal["execution", "__end__"]:
        """Route after execution phase."""
        if state["execution_status"] == "completed" or state["execution_status"] == "failed":
            return "__end__"
        elif state["execution_queue"]:
            return "execution"
        else:
            return "__end__"
    
    # Add conditional edges
    workflow.add_conditional_edges(
        "planning",
        route_after_planning,
        {
            "execution": "execution",
            "__end__": END
        }
    )
    
    workflow.add_conditional_edges(
        "execution",
        route_after_execution,
        {
            "execution": "execution",
            "__end__": END
        }
    )
    
    return workflow


# Example usage and testing
if __name__ == "__main__":
    from langchain_core.tools import tool
    
    # Mock tools for demonstration with proper schemas
    @tool
    def validate_data(data_source: str) -> str:
        """Validates data from the specified source."""
        print(f"üîç Validating data source: {data_source}")
        return f"Data validation completed for {data_source} - Status: Valid"
    
    @tool
    def check_database_connection(database_name: str) -> str:
        """Checks connection to the specified database."""
        print(f"üîó Checking database connection: {database_name}")
        return f"Database connection verified for {database_name} - Status: Connected"
    
    @tool
    def execute_query(query: str) -> str:
        """Executes a database query."""
        print(f"üíæ Executing query: {query}")
        return f"Query executed successfully: {query} - Rows affected: 150"
    
    @tool
    def send_notification(message: str, recipient: str) -> str:
        """Sends a notification message."""
        print(f"üìß Sending notification to {recipient}: {message}")
        return f"Notification sent to {recipient}: {message} - Status: Delivered"
    
    # Initialize LLM (replace with your preferred LLM)
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    
    # Available tools
    tools = [validate_data, check_database_connection, execute_query, send_notification]
    
    # Create the multi-agent workflow
    workflow = create_multi_agent_workflow(llm, tools)
    
    # Compile the graph
    app = workflow.compile()
    
    # Example SOP content
    sample_sop = """
    SOP for Error Code 935 - Data Validation Failure
    
    1. First, validate the incoming data source 'customer_data'
    2. If validation fails, check database connection to 'main_db'
    3. If connection is successful, execute repair query 'UPDATE customers SET status = active WHERE status IS NULL'
    4. If repair is successful, re-validate the data source
    5. If re-validation passes, send success notification to 'admin@company.com'
    6. If any step fails, send error notification with details to 'admin@company.com'
    """
    
    # Test the multi-agent system
    initial_state: AgentState = {
        "error_code": 935,
        "sop_content": sample_sop,
        "playbook": None,
        "execution_queue": [],
        "data": None,
        "last_tool_result": None,
        "final_output": {},
        "current_step_id": None,
        "execution_status": "planning",
        "execution_log": []
    }
    
    # Run the multi-agent workflow
    try:
        print("üöÄ Starting Enhanced Multi-Agent Workflow with Fixed Tool Execution...")
        print("=" * 70)
        
        result = app.invoke(initial_state)
        
        print("=" * 70)
        print("üìä Final Results:")
        print(f"Execution Status: {result['execution_status']}")
        print(f"Steps Executed: {len(result.get('execution_log', []))}")
        
        if result.get('execution_log'):
            print("\nüìã Detailed Execution Log:")
            for i, log_entry in enumerate(result['execution_log'], 1):
                step_result = log_entry['result']
                print(f"\n{i}. Step: {log_entry['step_name']} (ID: {log_entry['step_id']})")
                print(f"   Status: {step_result.get('status', 'unknown')}")
                print(f"   Message: {step_result.get('message', 'No message')}")
                
                if step_result.get('tool_used'):
                    print(f"   Tool Used: {step_result['tool_used']}")
                    print(f"   Tool Args: {step_result.get('tool_args', {})}")
                
                if step_result.get('tool_result'):
                    print(f"   Tool Result: {step_result['tool_result']}")
                
                if step_result.get('error'):
                    print(f"   ‚ùå Error: {step_result['error']}")
        
        if result.get('final_output'):
            print(f"\nüéØ Final Output: {result['final_output']}")
            
        # Display playbook that was created
        if result.get('playbook'):
            print(f"\nüìñ Playbook Created:")
            print(f"   Name: {result['playbook'].get('name', 'Unnamed')}")
            print(f"   Description: {result['playbook'].get('description', 'No description')}")
            print(f"   Total Steps: {len(result['playbook'].get('steps', {}))}")
            print(f"   Start Step: {result['playbook'].get('start_step', 'Unknown')}")
    
    except Exception as e:
        print(f"‚ùå Error running enhanced multi-agent workflow: {str(e)}")
        import traceback
        print("Full traceback:")
        traceback.print_exc()